{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and plotlib kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import warnings\n",
    "import Levenshtein as lev\n",
    "import tsfresh\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import mpldatacursor\n",
    "mpldatacursor.datacursor()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfresh\n",
    "tsfresh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tikzplotlib\n",
    "\n",
    "def save_tikzplotlib(fig,path):\n",
    "    def tikzplotlib_fix_ncols(obj):\n",
    "        \"\"\"\n",
    "        workaround for matplotlib 3.6 renamed legend's _ncol to _ncols, which breaks tikzplotlib\n",
    "        \"\"\"\n",
    "        if hasattr(obj, \"_ncols\"):\n",
    "            obj._ncol = obj._ncols\n",
    "        for child in obj.get_children():\n",
    "            tikzplotlib_fix_ncols(child)\n",
    "    tikzplotlib_fix_ncols(fig)\n",
    "    tikzplotlib.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping(version):\n",
    "    global device_name_mapping_simple\n",
    "    global device_name_mapping \n",
    "    global device_type_mapping \n",
    "    global devices_name    \n",
    "    global devices\n",
    "    \n",
    "    if version == 2:\n",
    "\n",
    "        devices = ['0x0000', '0x265e', '0x3181', '0x0a79', '0xb815', '0x4e52', '0x9989', '0x5db6', '0x772c', '0x61af', '0xe011', '0xe694','0x6e5f', '0xc8f0', '0x09ac', '0x82eb', '0x054f', '0xebe5',  '0xe5c4', '0x482d', '0xa209', '0x2cae'];\n",
    "\n",
    "        devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', \n",
    "                        'Aqara Button', 'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb',  'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "\n",
    "        device_type_mapping = {\n",
    "            '0x0000' : 'Coordinator',\n",
    "            '0x09ac' : 'Socket',\n",
    "            '0x5db6' : 'Door',\n",
    "            '0xe694' : 'Socket',\n",
    "            '0x772c' : 'Door',\n",
    "            '0x61af' : 'Vibration',\n",
    "            '0xe011' : 'Button',\n",
    "            '0x9989' : 'Motion',\n",
    "            '0x265e' : 'Temperature',\n",
    "            '0xb815' : 'Motion',\n",
    "            '0x4e52' : 'Motion',\n",
    "            '0x3181' : 'Door',\n",
    "            '0x0a79' : 'Door',\n",
    "            '0x82eb' : 'Socket',\n",
    "            '0x6e5f' : 'Socket',\n",
    "            '0xc8f0' : 'Socket',\n",
    "            '0xebe5' : 'Bulb',\n",
    "            '0x054f' : 'Bulb',\n",
    "            '0x482d' : 'Bulb',\n",
    "            '0xa209' : 'Bulb',\n",
    "            '0x2cae' : 'Motion',\n",
    "            '0xe5c4' : 'Bulb'\n",
    "        }\n",
    "\n",
    "        device_name_mapping = {\n",
    "            '0x0000' : 'Coordinator',\n",
    "            '0x09ac' : 'Ledvance Z3 Plug',\n",
    "            '0x5db6' : 'Aqara Door 1',\n",
    "            '0xe694' : 'Smart Socket',\n",
    "            '0x772c' : 'Aqara Door 2',\n",
    "            '0x61af' : 'Aqara Vibration',\n",
    "            '0xe011' : 'Aqara Button',\n",
    "            '0x9989' : 'Aqara Motion',\n",
    "            '0x265e' : 'Sonoff Temperature',\n",
    "            '0xb815' : 'Sonoff Motion 1',\n",
    "            '0x4e52' : 'Sonoff Motion 2',\n",
    "            '0x3181' : 'Sonoff Door 1',\n",
    "            '0x0a79' : 'Sonoff Door 2',\n",
    "            '0x82eb' : 'Ledvance Smart+ Plug',\n",
    "            '0x6e5f' : 'Power Plug 1',\n",
    "            '0xc8f0' : 'Power Plug 2',\n",
    "            '0xebe5' : 'Moes Bulb',\n",
    "            '0x054f' : 'Ledvance Bulb',\n",
    "            '0x482d' : 'Philips Lamp 1',\n",
    "            '0xa209' : 'Philips Lamp 2',\n",
    "            '0x2cae' : 'Philips Lamp 3',\n",
    "            '0xe5c4' : 'Philips Motion'\n",
    "        }\n",
    "\n",
    "        device_name_mapping_simple = {\n",
    "            '0x0000' : 'Coordinator',\n",
    "            '0x09ac' : 'Ledvance Z3 Plug',\n",
    "            '0x5db6' : 'Aqara Door',\n",
    "            '0xe694' : 'Smart Socket',\n",
    "            '0x772c' : 'Aqara Door',\n",
    "            '0x61af' : 'Aqara Vibration',\n",
    "            '0xe011' : 'Aqara Button',\n",
    "            '0x9989' : 'Aqara Motion',\n",
    "            '0x265e' : 'Sonoff Temperature',\n",
    "            '0xb815' : 'Sonoff Motion',\n",
    "            '0x4e52' : 'Sonoff Motion',\n",
    "            '0x3181' : 'Sonoff Door',\n",
    "            '0x0a79' : 'Sonoff Door',\n",
    "            '0x82eb' : 'Ledvance Smart+ Plug',\n",
    "            '0x6e5f' : 'Power Plug',\n",
    "            '0xc8f0' : 'Power Plug',\n",
    "            '0xebe5' : 'Moes Bulb',\n",
    "            '0x054f' : 'Ledvance Bulb',\n",
    "            '0x482d' : 'Philips Lamp',\n",
    "            '0xa209' : 'Philips Lamp',\n",
    "            '0x2cae' : 'Philips Lamp',\n",
    "            '0xe5c4' : 'Philips Motion'\n",
    "        }\n",
    "    else:\n",
    "        devices = ['0x0000', '0x4615', '0x946e', '0x7b10', '0xd0bb', '0x1f29', '0x27d7', '0x907b', '0xe01d', '0x187a', '0xc31c',\n",
    "        '0xe1a6', '0x3d95', '0xa706', '0x4e11', '0x0112', '0xec7f', '0x1e15', '0x1cd8', '0x5bb9', '0x711c', '0x059b']\n",
    "\n",
    "        devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', 'Aqara Button',\n",
    "                        'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb', 'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "        device_type_mapping = {\n",
    "            '0x0000': 'Coordinator',\n",
    "            '0x4615': 'Temperature',\n",
    "            '0x946e': 'Door',\n",
    "            '0x7b10': 'Door',\n",
    "            '0xd0bb': 'Motion',\n",
    "            '0x1f29': 'Motion',\n",
    "            '0x27d7': 'Motion',\n",
    "            '0x907b': 'Door',\n",
    "            '0xe01d': 'Door',\n",
    "            '0x187a': 'Vibration',\n",
    "            '0xc31c': 'Button',\n",
    "            '0xe1a6': 'Socket',\n",
    "            '0x3d95': 'Socket',\n",
    "            '0xa706': 'Socket',\n",
    "            '0x4e11': 'Socket',\n",
    "            '0x0112': 'Socket',\n",
    "            '0xec7f': 'Bulb',\n",
    "            '0x1e15': 'Bulb',\n",
    "            '0x1cd8': 'Bulb',\n",
    "            '0x5bb9': 'Bulb',\n",
    "            '0x711c': 'Bulb',\n",
    "            '0x059b': 'Motion'    \n",
    "        }\n",
    "\n",
    "        device_name_mapping = {\n",
    "            '0x0000': 'Coordinator',\n",
    "            '0x4615': 'Sonoff Temperature',\n",
    "            '0x946e': 'Sonoff Door 1',\n",
    "            '0x7b10': 'Sonoff Door 2',\n",
    "            '0xd0bb': 'Sonoff Motion 1',\n",
    "            '0x1f29': 'Sonoff Motion 2',\n",
    "            '0x27d7': 'Aqara Motion',\n",
    "            '0x907b': 'Aqara Door 1',\n",
    "            '0xe01d': 'Aqara Door 2',\n",
    "            '0x187a': 'Aqara Vibration',\n",
    "            '0xc31c': 'Aqara Button',\n",
    "            '0xe1a6': 'Smart Socket',\n",
    "            '0x3d95': 'Power Plug 1',\n",
    "            '0xa706': 'Power Plug 2',\n",
    "            '0x4e11': 'Ledvance Z3 Plug',\n",
    "            '0x0112': 'Ledvance Smart+ Plug',\n",
    "            '0xec7f': 'Ledvance Bulb',\n",
    "            '0x1e15': 'Moes Bulb',\n",
    "            '0x1cd8': 'Philips Lamp 1',\n",
    "            '0x5bb9': 'Philips Lamp 2',\n",
    "            '0x711c': 'Philips Lamp 3',\n",
    "            '0x059b': 'Philips Motion' \n",
    "        }\n",
    "\n",
    "        device_name_mapping_simple = {\n",
    "            '0x0000': 'Coordinator',\n",
    "            '0x4615': 'Sonoff Temperature',\n",
    "            '0x946e': 'Sonoff Door',\n",
    "            '0x7b10': 'Sonoff Door',\n",
    "            '0xd0bb': 'Sonoff Motion',\n",
    "            '0x1f29': 'Sonoff Motion',\n",
    "            '0x27d7': 'Aqara Motion',\n",
    "            '0x907b': 'Aqara Door',\n",
    "            '0xe01d': 'Aqara Door',\n",
    "            '0x187a': 'Aqara Vibration',\n",
    "            '0xc31c': 'Aqara Button',\n",
    "            '0xe1a6': 'Smart Socket',\n",
    "            '0x3d95': 'Power Plug',\n",
    "            '0xa706': 'Power Plug',\n",
    "            '0x4e11': 'Ledvance Z3 Plug',\n",
    "            '0x0112': 'Ledvance Smart+ Plug',\n",
    "            '0xec7f': 'Ledvance Bulb',\n",
    "            '0x1e15': 'Moes Bulb',\n",
    "            '0x1cd8': 'Philips Lamp',\n",
    "            '0x5bb9': 'Philips Lamp',\n",
    "            '0x711c': 'Philips Lamp',\n",
    "            '0x059b': 'Philips Motion' \n",
    "        }\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority ranking for commands\n",
    "\n",
    "command_priorities = {\n",
    "    'Sonoff Temperature': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Sonoff Door': {\n",
    "        'Zone Status Change Notification (0x00)': 3,\n",
    "        'Report Attributes (0x0a)': 2,\n",
    "    },\n",
    "\n",
    "    'Sonoff Motion': {\n",
    "        'Zone Status Change Notification (0x00)': 7,\n",
    "        'Report Attributes (0x0a)': 6,\n",
    "        'Zone Enroll Request (0x01)': 5,\n",
    "        'Rejoin Request (0x06)': 4,\n",
    "        'zdp': 3,\n",
    "        'Ack': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Aqara Motion': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Aqara Door': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Aqara Vibration': {\n",
    "        'Report Attributes (0x0a)': 5,\n",
    "        'Rejoin Request (0x06)': 4,\n",
    "        'zdp': 3,\n",
    "        'Ack': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Aqara Button': {\n",
    "        'Report Attributes (0x0a)': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Smart Socket': {\n",
    "        'Report Attributes (0x0a)': 9,\n",
    "        'Read Attributes Response (0x01)': 9,\n",
    "        'Read Attributes (0x00)': 8,\n",
    "        'Default Response (0x0b)': 7,\n",
    "        'APS: Ack': 6,\n",
    "        'Ack': 5,\n",
    "        'Rejoin Response (0x07)': 4,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'zdp': 2,\n",
    "        'Link Status': 1,\n",
    "    },\n",
    "\n",
    "    'Power Plug': {\n",
    "        'Read Attribute Response (0x01)': 9,\n",
    "        'Report Attributes (0x0a)': 9,\n",
    "        'Read Attributes (0x00)': 8,\n",
    "        'Default Response (0x0b)': 7,\n",
    "        'APS: Ack': 6,\n",
    "        'Ack': 5,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'zdp': 2,\n",
    "        'Link Status': 1,\n",
    "    },\n",
    "\n",
    "    'Ledvance Z3 Plug': {\n",
    "        'Link Status': 1,\n",
    "        'Route Record (0x05)': 6,\n",
    "        'Ack': 7,\n",
    "        'Report Attributes (0x0a)': 10,\n",
    "        'APS: Ack': 8,\n",
    "        'zdp': 2,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Get Group Membership Response (0x02)': 5,\n",
    "    },\n",
    "\n",
    "    'Ledvance Smart+ Plug': {\n",
    "        'Link Status': 2,\n",
    "        'Report Attributes (0x0a)': 10,\n",
    "        'Ack': 7,\n",
    "        'zdp': 3,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 4,\n",
    "        '---': 1,\n",
    "        'Get Group Membership Response (0x02)': 6,\n",
    "        'APS: Ack': 8,\n",
    "        'Rejoin Response (0x07)': 5,\n",
    "    },\n",
    "\n",
    "    'Ledvance Bulb': {\n",
    "        'Link Status': 1,\n",
    "        'Ack': 7,\n",
    "        'Route Record (0x05)': 6,\n",
    "        'APS: Ack': 8,\n",
    "        'zdp': 2,\n",
    "        'Report Attributes (0x0a)': 11,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Get Group Membership Response (0x02)': 5,\n",
    "    },\n",
    "\n",
    "    'Moes Bulb': {\n",
    "        'Link Status': 1,\n",
    "        'Ack': 4,\n",
    "        'Default Response (0x0b)': 8,\n",
    "        'Read Attribute Response (0x01)': 6,\n",
    "        'Report Attributes (0x0a)': 7,\n",
    "        'APS: Ack': 5,\n",
    "        'zdp': 2,\n",
    "        'Route Reply (0x02)': 3,\n",
    "    },\n",
    "\n",
    "    'Philips Lamp': {\n",
    "        'Link Status': 2,\n",
    "        'Route Record (0x05)': 7,\n",
    "        'zdp': 3,\n",
    "        'Ack': 8,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Report Attributes (0x0a)': 11,\n",
    "        'APS: Ack': 9,\n",
    "        'Default Response (0x0b)': 12,\n",
    "        'Get Group Membership Response (0x02)': 6,\n",
    "        'Route Reply (0x02)': 5,\n",
    "        'Rejoin Response (0x07)': 4,\n",
    "        '---': 1,\n",
    "    },\n",
    "\n",
    "    'Philips Motion': {\n",
    "        'Report Attributes (0x0a)': 6,\n",
    "        'APS: Ack': 5,\n",
    "        'Route Record (0x05)': 2,\n",
    "        'Ack': 4,\n",
    "        'zdp': 1,\n",
    "        'Rejoin Request (0x06)': 3,\n",
    "    },\n",
    "\n",
    "    'Coordinator': {\n",
    "        'Read Attributes (0x00)': 11,\n",
    "        'Ack': 7,\n",
    "        'APS: Ack': 8,\n",
    "        'Link Status': 1,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'zdp': 2,\n",
    "        'On (0x01)': 12,\n",
    "        'Move To Level with OnOff (0x04)': 14,\n",
    "        'Off (0x00)': 12,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Move To Color (0x07)': 13,\n",
    "        'Get Group Membership (0x02)': 4,\n",
    "        'Move To Color Temperature (0x0a)': 14,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Configure Reporting (0x06)': 5,\n",
    "        'Color Loop Set (0x44)': 15,\n",
    "        'Rejoin Response (0x07)': 6,\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_highest_priority_action(device_name, action_list):\n",
    "    if device_name in command_priorities:\n",
    "        # Get the priorities for the specified device\n",
    "        device_priorities = command_priorities[device_name]\n",
    "\n",
    "        # Filter the action list to include only those present in the priorities\n",
    "        valid_actions = [action for action in action_list if action in device_priorities]\n",
    "\n",
    "        if valid_actions:\n",
    "            # Find the action with the highest priority\n",
    "            highest_priority_action = max(valid_actions, key=lambda action: device_priorities[action])\n",
    "            return highest_priority_action\n",
    "        else:\n",
    "            return '---'  # No valid actions found in the priorities\n",
    "    else:\n",
    "        return None  # Device not found in the command_priorities dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load acquisition 2\n",
    "\n",
    "df = pd.read_csv(r'../final_dataFrame_v2_anto.csv')\n",
    "\n",
    "ground_truth_df = pd.read_csv(r'../ground_truth_v2.csv')\n",
    "ground_truth_df = ground_truth_df.rename(columns={'0': 'Action'})\n",
    "if 'Unnamed: 0' in ground_truth_df.columns:\n",
    "    ground_truth_df = ground_truth_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)\n",
    "\n",
    "devices = ['0x0000', '0x265e', '0x3181', '0x0a79', '0xb815', '0x4e52', '0x9989', '0x5db6', '0x772c', '0x61af', '0xe011', '0xe694',\n",
    "            '0x6e5f', '0xc8f0', '0x09ac', '0x82eb', '0x054f', '0xebe5',  '0xe5c4', '0x482d', '0xa209', '0x2cae']\n",
    "\n",
    "devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', \n",
    "                'Aqara Button', 'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb',  'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "device_type_mapping = {\n",
    "    '0x0000' : 'Coordinator',\n",
    "    '0x09ac' : 'Socket',\n",
    "    '0x5db6' : 'Door',\n",
    "    '0xe694' : 'Socket',\n",
    "    '0x772c' : 'Door',\n",
    "    '0x61af' : 'Vibration',\n",
    "    '0xe011' : 'Button',\n",
    "    '0x9989' : 'Motion',\n",
    "    '0x265e' : 'Temperature',\n",
    "    '0xb815' : 'Motion',\n",
    "    '0x4e52' : 'Motion',\n",
    "    '0x3181' : 'Door',\n",
    "    '0x0a79' : 'Door',\n",
    "    '0x82eb' : 'Socket',\n",
    "    '0x6e5f' : 'Socket',\n",
    "    '0xc8f0' : 'Socket',\n",
    "    '0xebe5' : 'Bulb',\n",
    "    '0x054f' : 'Bulb',\n",
    "    '0x482d' : 'Bulb',\n",
    "    '0xa209' : 'Bulb',\n",
    "    '0x2cae' : 'Motion',\n",
    "    '0xe5c4' : 'Bulb'\n",
    "}\n",
    "\n",
    "device_name_mapping = {\n",
    "    '0x0000' : 'Coordinator',\n",
    "    '0x09ac' : 'Ledvance Z3 Plug',\n",
    "    '0x5db6' : 'Aqara Door',\n",
    "    '0xe694' : 'Smart Socket',\n",
    "    '0x772c' : 'Aqara Door',\n",
    "    '0x61af' : 'Aqara Vibration',\n",
    "    '0xe011' : 'Aqara Button',\n",
    "    '0x9989' : 'Aqara Motion',\n",
    "    '0x265e' : 'Sonoff Temperature',\n",
    "    '0xb815' : 'Sonoff Motion',\n",
    "    '0x4e52' : 'Sonoff Motion',\n",
    "    '0x3181' : 'Sonoff Door',\n",
    "    '0x0a79' : 'Sonoff Door',\n",
    "    '0x82eb' : 'Ledvance Smart+ Plug',\n",
    "    '0x6e5f' : 'Power Plug',\n",
    "    '0xc8f0' : 'Power Plug',\n",
    "    '0xebe5' : 'Moes Bulb',\n",
    "    '0x054f' : 'Ledvance Bulb',\n",
    "    '0x482d' : 'Philips Lamp',\n",
    "    '0xa209' : 'Philips Lamp',\n",
    "    '0x2cae' : 'Philips Lamp',\n",
    "    '0xe5c4' : 'Philips Motion'\n",
    "}\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=71482)\n",
    "threshold_float = 0.071482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Extended Source'])\n",
    "y = df['Extended Source']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.to_csv('train.csv', index=False)\n",
    "X_test.to_csv('test.csv', index=False)\n",
    "y_train.to_csv('train_labels.csv', index=False)\n",
    "y_test.to_csv('test_labels.csv', index=False)\n",
    "\n",
    "X_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "y_test = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add source zigbee and destination zigbee to ack packets\n",
    "seq_no = df.loc[0,'Sequence Number']\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    seq_no1 = df.loc[i,'Sequence Number']\n",
    "    if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "        df.loc[i,'Source Zigbee'] = df.loc[i-1,'Destination Zigbee']\n",
    "        df.loc[i,'Destination Zigbee'] = df.loc[i-1,'Source Zigbee']\n",
    "    seq_no = seq_no1\n",
    "    if(np.isnan(df.loc[i, 'Payload Length'])):\n",
    "        df.loc[i, 'Payload Length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_dataFrame_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframes 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load acquisition 4\n",
    "\n",
    "df = pd.read_csv(r'../final_dataFrame_v4_anto.csv')\n",
    "\n",
    "ground_truth_df = pd.read_csv(r'../ground_truth_v4.csv')\n",
    "ground_truth_df = ground_truth_df.rename(columns={'0': 'Action'})\n",
    "if 'Unnamed: 0' in ground_truth_df.columns:\n",
    "    ground_truth_df = ground_truth_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)\n",
    "\n",
    "devices = ['0x0000', '0x4615', '0x946e', '0x7b10', '0xd0bb', '0x1f29', '0x27d7', '0x907b', '0xe01d', '0x187a', '0xc31c',\n",
    "           '0xe1a6', '0x3d95', '0xa706', '0x4e11', '0x0112', '0xec7f', '0x1e15', '0x1cd8', '0x5bb9', '0x711c', '0x059b']\n",
    "\n",
    "devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', 'Aqara Button',\n",
    "                'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb', 'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "device_type_mapping = {\n",
    "    '0x0000': 'Coordinator',\n",
    "    '0x4615': 'Temperature',\n",
    "    '0x946e': 'Door',\n",
    "    '0x7b10': 'Door',\n",
    "    '0xd0bb': 'Motion',\n",
    "    '0x1f29': 'Motion',\n",
    "    '0x27d7': 'Motion',\n",
    "    '0x907b': 'Door',\n",
    "    '0xe01d': 'Door',\n",
    "    '0x187a': 'Vibration',\n",
    "    '0xc31c': 'Button',\n",
    "    '0xe1a6': 'Socket',\n",
    "    '0x3d95': 'Socket',\n",
    "    '0xa706': 'Socket',\n",
    "    '0x4e11': 'Socket',\n",
    "    '0x0112': 'Socket',\n",
    "    '0xec7f': 'Bulb',\n",
    "    '0x1e15': 'Bulb',\n",
    "    '0x1cd8': 'Bulb',\n",
    "    '0x5bb9': 'Bulb',\n",
    "    '0x711c': 'Bulb',\n",
    "    '0x059b': 'Motion'    \n",
    "}\n",
    "\n",
    "device_name_mapping = {\n",
    "    '0x0000': 'Coordinator',\n",
    "    '0x4615': 'Sonoff Temperature',\n",
    "    '0x946e': 'Sonoff Door',\n",
    "    '0x7b10': 'Sonoff Door',\n",
    "    '0xd0bb': 'Sonoff Motion',\n",
    "    '0x1f29': 'Sonoff Motion',\n",
    "    '0x27d7': 'Aqara Motion',\n",
    "    '0x907b': 'Aqara Door',\n",
    "    '0xe01d': 'Aqara Door',\n",
    "    '0x187a': 'Aqara Vibration',\n",
    "    '0xc31c': 'Aqara Button',\n",
    "    '0xe1a6': 'Smart Socket',\n",
    "    '0x3d95': 'Power Plug',\n",
    "    '0xa706': 'Power Plug',\n",
    "    '0x4e11': 'Ledvance Z3 Plug',\n",
    "    '0x0112': 'Ledvance Smart+ Plug',\n",
    "    '0xec7f': 'Ledvance Bulb',\n",
    "    '0x1e15': 'Moes Bulb',\n",
    "    '0x1cd8': 'Philips Lamp',\n",
    "    '0x5bb9': 'Philips Lamp',\n",
    "    '0x711c': 'Philips Lamp',\n",
    "    '0x059b': 'Philips Motion' \n",
    "}\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=63446)\n",
    "threshold_float = 0.063446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Extended Source'])\n",
    "y = df['Extended Source']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.to_csv('train.csv', index=False)\n",
    "X_test.to_csv('test.csv', index=False)\n",
    "y_train.to_csv('train_labels.csv', index=False)\n",
    "y_test.to_csv('test_labels.csv', index=False)\n",
    "\n",
    "X_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "y_test = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add source zigbee and destination zigbee to ack packets\n",
    "seq_no = df.loc[0,'Sequence Number']\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    seq_no1 = df.loc[i,'Sequence Number']\n",
    "    if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "        df.loc[i,'Source Zigbee'] = df.loc[i-1,'Destination Zigbee']\n",
    "        df.loc[i,'Destination Zigbee'] = df.loc[i-1,'Source Zigbee']\n",
    "    seq_no = seq_no1\n",
    "    if(np.isnan(df.loc[i, 'Payload Length'])):\n",
    "        df.loc[i, 'Payload Length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_dataFrame_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAT Density Plot V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = True\n",
    "MICROS = 17.6*1000000 #100000\n",
    "df = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "SOURCE = 'Source IEEE' #Source IEEE Zigbee\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df = df[df['File'].str.contains('idle')]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Calculate the time delta between packets with the same sequence number (packet and corresponding ack)\n",
    "# df.sort_values(by=['Time'])\n",
    "# seq_no = df.loc[0,'Sequence Number']\n",
    "# date_string = df.loc[0,'Time']\n",
    "# date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "# date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "# list_of_timedelta = []\n",
    "\n",
    "# for i in range(1, len(df)):\n",
    "#     seq_no1 = df.loc[i,'Sequence Number']\n",
    "#     date_string = df.loc[i,'Time']\n",
    "#     date_object1 = datetime.strptime(date_string, date_format)\n",
    "#     if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "#         list_of_timedelta.append(date_object1 - date_object)\n",
    "#     date_object = date_object1\n",
    "#     seq_no = seq_no1\n",
    "\n",
    "# threshold = timedelta(days=0, hours=0, seconds=0, microseconds=20000)\n",
    "# second_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=0)\n",
    "# filtered_timedelta = [td for td in list_of_timedelta if td < threshold]\n",
    "# filtered_timedelta = [td for td in filtered_timedelta if td > second_threshold]\n",
    "# filtered_timedelta_micro = [td.microseconds for td in filtered_timedelta]\n",
    "\n",
    "# plt.hist(filtered_timedelta_micro, bins=100, edgecolor='k')\n",
    "# plt.xlabel('Time in Microseconds')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Timedelta Values for Ack Packets')\n",
    "# plt.grid(True)\n",
    "\n",
    "# hist, bin_edges = np.histogram(filtered_timedelta_micro, bins=100)\n",
    "# cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "# percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "# percentile_value = bin_edges[percentile_bin + 1]\n",
    "# print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "# number_of_ack = len(df[df['FCF IEEE'] == '0x0002'])\n",
    "# print(f\"The total number of Ack in the DataFrame is {number_of_ack}\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the time delta between packets\n",
    "df.sort_values(by=['Time'])\n",
    "date_string = df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "\n",
    "for dev in df[SOURCE].unique():\n",
    "#    print(dev)\n",
    "    df_dev = df[df[SOURCE] == dev]\n",
    "    if len(df_dev) >0:\n",
    "        df_dev.reset_index(drop=True, inplace=True)\n",
    "        date_string = df_dev.loc[0]['Time']\n",
    "        date_object = datetime.strptime(date_string, date_format)\n",
    "        for i in range(1, len(df_dev)):\n",
    "            date_string = df_dev.iloc[i]['Time']\n",
    "            date_object1 = datetime.strptime(date_string, date_format)\n",
    "            list_of_timedelta.append(date_object1 - date_object)\n",
    "            date_object = date_object1\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=150000)\n",
    "second_threshold = timedelta(days=0)\n",
    "\n",
    "filtered_timedelta = [td for td in list_of_timedelta if td < threshold and td > second_threshold]\n",
    "\n",
    "filtered_timedelta_milli = [td.total_seconds()*1000 for td in filtered_timedelta]\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.hist(filtered_timedelta_milli, bins=60, edgecolor='k')\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values')\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_milli, bins=60)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "percentile_bin = np.where(cdf > 0.97)[0][0]\n",
    "\n",
    "percentile_value = bin_edges[percentile_bin + 1]\n",
    "print(f\"The 99th percentile value is {percentile_value} Milliseconds\")\n",
    "\n",
    "import statistics\n",
    "mean = statistics.mean(filtered_timedelta_milli)\n",
    "std = statistics.stdev(filtered_timedelta_milli)\n",
    "\n",
    "print (f\"Mean: {mean}, Std: {std}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "## Density plot\n",
    "\n",
    "sns.kdeplot(filtered_timedelta_milli, bw_method=0.1)\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Timedelta Values')\n",
    "#save_tikzplotlib(plt.gcf(), \"threshold.tex\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the time delta between packets ingoing or outgoing for a specific device\n",
    "\n",
    "device_address = '0x265e'\n",
    "\n",
    "device_df = df[(df[SOURCE] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "print(f\"{len(device_df)} packets for device {device_address}\")\n",
    "\n",
    "device_df.sort_values(by=['Time'])\n",
    "device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "date_string = device_df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(device_df)):\n",
    "    date_string = device_df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=150000)\n",
    "second_threshold = timedelta(days=0)\n",
    "filtered_timedelta_ledvance = [td for td in list_of_timedelta if td < threshold]\n",
    "filtered_timedelta_ledvance = [td for td in filtered_timedelta_ledvance if td > second_threshold]\n",
    "filtered_timedelta_micro_ledvance = [td.microseconds for td in filtered_timedelta_ledvance]\n",
    "\n",
    "plt.hist(filtered_timedelta_micro_ledvance, bins=60, edgecolor='k')\n",
    "plt.xlabel('Time in Microseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values')\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_micro_ledvance, bins=60)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "percentile_value = bin_edges[percentile_bin + 1]\n",
    "print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Density plot based on inter arrival time\n",
    "\n",
    "colormap = plt.cm.get_cmap('tab20', len(devices))\n",
    "\n",
    "devices_hist_v2 = []\n",
    "for device_address in devices:\n",
    "    if device_address == '0xe011':\n",
    "        device_df = df[(df[SOURCE] == device_address)]\n",
    "        print(device_df)\n",
    "    \n",
    "\n",
    "    #device_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_df = df[(df[SOURCE] == device_address)]\n",
    "    device_df = device_df.sort_values(by=['Time'])\n",
    "    print(device_address)\n",
    "    print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "    device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    date_string = device_df.loc[0,'Time']\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(device_df)):\n",
    "        date_string = device_df.loc[i,'Time']\n",
    "        date_object1 = datetime.strptime(date_string, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=0, microseconds=MICROS) and td > timedelta(days=0)]\n",
    "    print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "    filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "    if len(filtered_timedelta_micro_device) > 0:\n",
    "        print(f\"{max(filtered_timedelta_micro_device)} microseconds\")\n",
    "\n",
    "\n",
    "    #plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "    sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address, color=colormap(devices.index(device_address)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time in Milliseconds')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f\"Histogram of Timedelta Values\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=300, range=(0,100))\n",
    "    devices_hist_v2.append(hist)\n",
    "    '''\n",
    "    cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "    percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "    percentile_value = bin_edges[percentile_bin + 1]\n",
    "    print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "    '''\n",
    "    print('')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"device_density.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df['Device Type'] = df[SOURCE].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab10', len(df['Device Type'].unique()))\n",
    "\n",
    "quant_list = list()\n",
    "quant_raw_list = list()\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    # if device_type == 'Coordinator' or device_type == 'Button' or device_type == 'Vibration': \n",
    "    #     continue\n",
    "\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_milli_device = []\n",
    "    \n",
    "    print(f\"Device type: {device_type}\")\n",
    "\n",
    "    for device_address in subset_df[SOURCE].unique():\n",
    "        device_df = subset_df[subset_df[SOURCE] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=17.160, microseconds=0) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_milli_device = [td.total_seconds()*1 for td in filtered_timedelta_device]\n",
    "        \n",
    "        print(f\"Device address: {device_address}, Number of packets filtered: {len(filtered_timedelta_milli_device)}, Number of packets: {len(list_of_timedelta)}, MEAN: {statistics.mean(filtered_timedelta_micro_device)}\")\n",
    "        \n",
    "\n",
    "        \n",
    "        all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "\n",
    "\n",
    "    # if len(all_timedelta_milli_device) < 10:\n",
    "    #     continue\n",
    "    \n",
    "    print(f\"Number of packets: {len(all_timedelta_milli_device)}\")\n",
    "    \n",
    "    quant_raw_list.extend(all_timedelta_milli_device)\n",
    "    all_timedelta_micro_device_pd = pd.Series(all_timedelta_milli_device)\n",
    "    quant = all_timedelta_micro_device_pd.quantile(0.95)\n",
    "    if quant is not None and not np.isnan(quant):\n",
    "        quant_list.append(quant)\n",
    "        \n",
    "    print(all_timedelta_micro_device_pd.quantile(0.95))\n",
    "    \n",
    "    \n",
    "    \n",
    "    sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "    #sns.histplot(all_timedelta_micro_device, bins='auto',stat='count', kde=True, color=color, label=device_type,alpha=0.5)\n",
    "    #sns.displot(all_timedelta_micro_device, kind=\"kde\",color=color, label=device_type)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Median 95 Q {statistics.median(quant_list)}\")\n",
    "print(f\"Mean 95 Q {statistics.mean(quant_list)}\")\n",
    "q_raw = pd.Series(quant_raw_list).quantile(0.95)\n",
    "print(f\"Qraw 95 Q {q_raw}\")\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Seconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "save_tikzplotlib(plt.gcf(), \"category_density_idle_17.16s_no_coordinator_ieee_and_zb.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAT Density Plot V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = True\n",
    "SOURCE = 'Source IEEE' #Source IEEE Zigbee\n",
    "MICROS = 17.6*1000000 #100000\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Calculate the time delta between packets\n",
    "\n",
    "# df = df4\n",
    "# load_mapping(4)\n",
    "\n",
    "# df.sort_values(by=['Time'])\n",
    "# date_string = df.loc[0,'Time']\n",
    "# date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "# date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "# list_of_timedelta = []\n",
    "\n",
    "# for i in range(1, len(df)):\n",
    "#     date_string = df.loc[i,'Time']\n",
    "#     date_object1 = datetime.strptime(date_string, date_format)\n",
    "#     if True:\n",
    "#         list_of_timedelta.append(date_object1 - date_object)\n",
    "#     date_object = date_object1\n",
    "\n",
    "# threshold = timedelta(days=0, hours=0, seconds=0, microseconds=200000)\n",
    "# second_threshold = timedelta(days=0)\n",
    "# filtered_timedelta_no_ack = [td for td in list_of_timedelta if td < threshold]\n",
    "# filtered_timedelta_no_ack = [td for td in filtered_timedelta_no_ack if td > second_threshold]\n",
    "# filtered_timedelta_micro_no_ack = [td.microseconds for td in filtered_timedelta_no_ack]\n",
    "\n",
    "# # plt.hist(filtered_timedelta_micro_no_ack, bins=60, edgecolor='k')\n",
    "# # plt.xlabel('Time in Microseconds')\n",
    "# # plt.ylabel('Frequency')\n",
    "# # plt.title('Histogram of Timedelta Values')\n",
    "# # plt.grid(True)\n",
    "\n",
    "# # hist, bin_edges = np.histogram(filtered_timedelta_micro_no_ack, bins=60)\n",
    "# # cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "# # percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "# # percentile_value = bin_edges[percentile_bin + 1]\n",
    "# # print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# ## Density plot\n",
    "\n",
    "# sns.kdeplot(filtered_timedelta_micro_no_ack, bw_method=0.1)\n",
    "# plt.xlabel('Time in Microseconds')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Density Plot of Timedelta Values')\n",
    "# #save_tikzplotlib(plt.gcf(), \"threshold.tex\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Density plot based on inter arrival time per device\n",
    "\n",
    "df = df4\n",
    "load_mapping(4)\n",
    "\n",
    "colormap = plt.cm.get_cmap('tab20', len(devices))\n",
    "\n",
    "devices_hist_v4 = []\n",
    "for device_address in devices:\n",
    "    try:\n",
    "        #device_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "        device_df = df[(df[SOURCE] == device_address)]\n",
    "        device_df = device_df.sort_values(by=['Time'])\n",
    "        print(device_address)\n",
    "        print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        date_string = device_df.loc[0,'Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        date_object = datetime.strptime(date_string, date_format)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(device_df)):\n",
    "        date_string = device_df.loc[i,'Time']\n",
    "        date_object1 = datetime.strptime(date_string, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=0, microseconds=MICROS) and td > timedelta(days=0)]\n",
    "    print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "    filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "    if len(filtered_timedelta_micro_device) > 0:\n",
    "        print(f\"{max(filtered_timedelta_micro_device)} microseconds\")\n",
    "\n",
    "\n",
    "    #plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "    sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time in Milliseconds')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f\"Histogram of Timedelta Values\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=300, range=(0,100))\n",
    "    devices_hist_v4.append(hist)\n",
    "    '''\n",
    "    cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "    percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "    percentile_value = bin_edges[percentile_bin + 1]\n",
    "    print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "    '''\n",
    "    print('')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"device_density.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df = df4\n",
    "load_mapping(4)\n",
    "\n",
    "df['Device Type'] = df[SOURCE].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_micro_device = []\n",
    "\n",
    "    for device_address in subset_df[SOURCE].unique():\n",
    "        device_df = subset_df[subset_df[SOURCE] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=20, microseconds=10000) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "\n",
    "        all_timedelta_micro_device.extend(filtered_timedelta_micro_device)\n",
    "\n",
    "    sns.kdeplot(all_timedelta_micro_device, bw_method=0.1, label=device_type, color=color)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "#save_tikzplotlib(plt.gcf(), \"category_density_updated.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CosSimilarity Distance Prob Dist Idel State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract 300 bins from the kde plot\n",
    "def extract_bins (df,device_mapping=device_type_mapping):\n",
    "\n",
    "    df['Device Type'] = df['Source IEEE'].map(device_mapping)\n",
    "    \n",
    "    areas_per_device = {}\n",
    "    kde_list = []\n",
    "    # Density plot based on inter-arrival time for each category\n",
    "    colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "    for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "        if device_type == 'Coordinator' or device_type == 'nan' or device_type is None or device_type is np.nan:\n",
    "            continue\n",
    "\n",
    "        subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "        all_timedelta_milli_device = []\n",
    "        \n",
    "        print(f\"Device type: {device_type}\")\n",
    "\n",
    "        for device_address in subset_df['Source IEEE'].unique():\n",
    "            device_df = subset_df[subset_df['Source IEEE'] == device_address]\n",
    "            device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            list_of_timedelta = []\n",
    "            for i in range(1, len(device_df)):\n",
    "                date_string = device_df.loc[i, 'Time']\n",
    "                date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "                list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "            filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=17.160, microseconds=0) and td > timedelta(days=0)]\n",
    "\n",
    "            filtered_timedelta_milli_device = [td.total_seconds()*1000 for td in filtered_timedelta_device]\n",
    "            \n",
    "            \n",
    "            all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "            \n",
    "        \n",
    "        if len(all_timedelta_milli_device) <= 5:\n",
    "            continue\n",
    "        \n",
    "        plt.figure()\n",
    "        kde_plot = sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "        kde_list.append({'device':device_type,'kde':kde_plot})\n",
    "    \n",
    "    glob_min = 0\n",
    "    glob_max = 0\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        # Extract KDE Data:\n",
    "        kde = elem['kde']\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = x_values.min(), x_values.max()\n",
    "        if glob_max < x_max:\n",
    "            glob_max = x_max\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        kde = elem['kde']\n",
    "        # Extract KDE Data:\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = glob_min,glob_max\n",
    "        bin_width = (x_max - x_min) / 300\n",
    "        bins = np.linspace(x_min, x_max, 301)  # 301 points for 300 intervals\n",
    "\n",
    "        # Area Calculation:\n",
    "        areas = []\n",
    "        for i in range(len(bins) - 1):\n",
    "            bin_start, bin_end = bins[i], bins[i + 1]\n",
    "            bin_midpoint = (bin_start + bin_end) / 2\n",
    "\n",
    "            # Use Kernel Density Estimation Function Directly (no need for interpolation):\n",
    "            density = kde_line.get_ydata()[np.argmin(np.abs(x_values - bin_midpoint))]\n",
    "            area = density * bin_width\n",
    "            areas.append(area)\n",
    "\n",
    "        areas_per_device[elem['device']] = areas\n",
    "        \n",
    "    # for device_type, areas in areas_per_device.items():\n",
    "        # plt.figure()\n",
    "        # plt.bar(bins[:-1], areas, width=bin_width, align='edge', edgecolor='k', alpha=0.5)\n",
    "        # plt.xlabel('Time Interval (ms)')\n",
    "        # plt.ylabel('Area')\n",
    "        # plt.title(f'Areas under KDE for {device_type}')\n",
    "        # plt.show()\n",
    "    \n",
    "    return areas_per_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hist2 = extract_bins(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "hist4 = extract_bins(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = dict(sorted(hist2.items()))\n",
    "hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "print(hist2.keys())\n",
    "print(hist4.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wasserstein_distance\n",
    "# from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "# hist2_clean = hist2.copy()\n",
    "\n",
    "# if \"Vibration\" in hist2_clean:\n",
    "#     del hist2_clean[\"Vibration\"]\n",
    "\n",
    "# # Ensure Dictionaries are Sorted for Consistent Order\n",
    "# sorted_hist2 = dict(sorted(hist2_clean.items()))\n",
    "# sorted_hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "# # Assuming hist2 and hist4 are dictionaries of histograms\n",
    "# distance_matrix_total = np.zeros((len(sorted_hist4), len(sorted_hist2)))  # Use float dtype for EMD\n",
    "\n",
    "# # Compute pairwise EMD between histograms of the same device type\n",
    "# for i, (dev_type1, hist1_val) in enumerate(sorted_hist2.items()):\n",
    "#     for j, (dev_type2, hist2_val) in enumerate(sorted_hist4.items()):\n",
    "#         distance_matrix_total[j, i] = wasserstein_distance(hist1_val, hist2_val)  # Correct indexing\n",
    "\n",
    "# print(distance_matrix_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wasserstein_distance\n",
    "\n",
    "# # Assuming hist2 and hist4 are dictionaries of histograms\n",
    "# distance_matrix_total = np.zeros((len(hist2), len(hist2)))  # Use float dtype for EMD\n",
    "\n",
    "# # Compute pairwise EMD between histograms of the same device type\n",
    "# for i, (dev_type1, hist1_val) in enumerate(hist2.items()):\n",
    "#     for j, (dev_type2, hist2_val) in enumerate(hist2.items()):\n",
    "#         distance_matrix_total[j, i] = wasserstein_distance(hist1_val, hist2_val)  # Correct indexing\n",
    "\n",
    "# print(distance_matrix_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist2_clean = hist2.copy()\n",
    "\n",
    "if \"Vibration\" in hist2_clean:\n",
    "    del hist2_clean[\"Vibration\"]\n",
    "\n",
    "# # Ensure Dictionaries are Sorted for Consistent Order\n",
    "# sorted_hist2 = dict(sorted(hist2_clean.items()))\n",
    "# sorted_hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist4), len(hist2_clean)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2_clean.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "        distance_matrix_total[j, i] =np.dot(hist1_val, hist2_val) / (np.linalg.norm(hist1_val) * np.linalg.norm(hist2_val))\n",
    "\n",
    "print(distance_matrix_total)\n",
    "\n",
    "sns.heatmap(distance_matrix_total, xticklabels=hist2_clean.keys(), yticklabels=hist4.keys(), linewidths=0.5, cmap=\"vlag\", square=True)\n",
    "plt.title('Cosine Similarity for Acquisition 2 vs 4')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2_vs_v4.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist2_clean), len(hist2_clean)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2_clean.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist2_clean.items()):\n",
    "        distance_matrix_total[j, i] =np.dot(hist1_val, hist2_val) / (np.linalg.norm(hist1_val) * np.linalg.norm(hist2_val))\n",
    "\n",
    "print(distance_matrix_total)\n",
    "\n",
    "sns.heatmap(distance_matrix_total, xticklabels=hist2_clean.keys(), yticklabels=hist2_clean.keys(), linewidths=0.5, cmap=\"vlag\", square=True)\n",
    "plt.title('Cosine Similarity for Acquisition 2')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wasserstein_distance\n",
    "# from scipy.stats import entropy\n",
    "\n",
    "# # Assuming hist2 and hist4 are dictionaries of histograms\n",
    "# distance_matrix_total = np.zeros((len(hist4), len(hist2)))  # Use float dtype for EMD\n",
    "\n",
    "# # Compute pairwise EMD between histograms of the same device type\n",
    "# for i, (dev_type1, hist1_val) in enumerate(hist2.items()):\n",
    "#     for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "#         kde_data1_normalized = hist1_val/np.sum(hist1_val)\n",
    "#         kde_data2_normalized = hist2_val/np.sum(hist2_val)\n",
    "        \n",
    "#         distance_matrix_total[j, i] = entropy(kde_data1_normalized, kde_data2_normalized)\n",
    "\n",
    "# print(distance_matrix_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity per Device full, idle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract 300 bins from the kde plot\n",
    "def extract_bins (df,device_mapping,seconds=17.6,microseconds=0):\n",
    "\n",
    "    df['Device Type'] = df['Source IEEE'].map(device_mapping)\n",
    "    \n",
    "    areas_per_device = {}\n",
    "    kde_list = []\n",
    "    # Density plot based on inter-arrival time for each category\n",
    "    colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "    for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "        if device_type == 'Coordinator' or device_type == 'nan' or device_type is None or device_type is np.nan:\n",
    "            continue\n",
    "\n",
    "        subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "        all_timedelta_milli_device = []\n",
    "        \n",
    "        print(f\"Device type: {device_type}\")\n",
    "\n",
    "        for device_address in subset_df['Source IEEE'].unique():\n",
    "            device_df = subset_df[subset_df['Source IEEE'] == device_address]\n",
    "            device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            list_of_timedelta = []\n",
    "            for i in range(1, len(device_df)):\n",
    "                date_string = device_df.loc[i, 'Time']\n",
    "                date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "                list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "            filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=seconds, microseconds=microseconds) and td > timedelta(days=0)]\n",
    "\n",
    "            filtered_timedelta_milli_device = [td.total_seconds()*1000 for td in filtered_timedelta_device]\n",
    "            \n",
    "            \n",
    "            all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "            \n",
    "        \n",
    "        if len(all_timedelta_milli_device) <= 5:\n",
    "            continue\n",
    "        \n",
    "        plt.figure()\n",
    "        kde_plot = sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "        kde_list.append({'device':device_type,'kde':kde_plot})\n",
    "    \n",
    "    glob_min = 0\n",
    "    glob_max = 0\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        # Extract KDE Data:\n",
    "        kde = elem['kde']\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = x_values.min(), x_values.max()\n",
    "        if glob_max < x_max:\n",
    "            glob_max = x_max\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        kde = elem['kde']\n",
    "        # Extract KDE Data:\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = glob_min,glob_max\n",
    "        bin_width = (x_max - x_min) / 300\n",
    "        bins = np.linspace(x_min, x_max, 301)  # 301 points for 300 intervals\n",
    "\n",
    "        # Area Calculation:\n",
    "        areas = []\n",
    "        for i in range(len(bins) - 1):\n",
    "            bin_start, bin_end = bins[i], bins[i + 1]\n",
    "            bin_midpoint = (bin_start + bin_end) / 2\n",
    "\n",
    "            # Use Kernel Density Estimation Function Directly (no need for interpolation):\n",
    "            density = kde_line.get_ydata()[np.argmin(np.abs(x_values - bin_midpoint))]\n",
    "            area = density * bin_width\n",
    "            areas.append(area)\n",
    "\n",
    "        areas_per_device[elem['device']] = areas\n",
    "        \n",
    "    # for device_type, areas in areas_per_device.items():\n",
    "        # plt.figure()\n",
    "        # plt.bar(bins[:-1], areas, width=bin_width, align='edge', edgecolor='k', alpha=0.5)\n",
    "        # plt.xlabel('Time Interval (ms)')\n",
    "        # plt.ylabel('Area')\n",
    "        # plt.title(f'Areas under KDE for {device_type}')\n",
    "        # plt.show()\n",
    "    \n",
    "    return areas_per_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hist2 = extract_bins(df2,device_name_mapping,0,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "hist4 = extract_bins(df4,device_name_mapping,0,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = dict(sorted(hist2.items()))\n",
    "hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "print(hist2.keys())\n",
    "print(hist4.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hist2_clean = hist2.copy()\n",
    "\n",
    "# Filter hist2 to contain only keys that are also in hist4\n",
    "hist2_clean = {key: hist2[key] for key in hist4.keys() if key in hist2}\n",
    "\n",
    "# # Ensure Dictionaries are Sorted for Consistent Order\n",
    "# sorted_hist2 = dict(sorted(hist2_clean.items()))\n",
    "# sorted_hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist4), len(hist2_clean)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2_clean.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "        distance_matrix_total[j, i] =np.dot(hist1_val, hist2_val) / (np.linalg.norm(hist1_val) * np.linalg.norm(hist2_val))\n",
    "\n",
    "print(distance_matrix_total)\n",
    "\n",
    "\n",
    "sns.heatmap(distance_matrix_total, xticklabels=hist2_clean.keys(), yticklabels=hist4.keys(), linewidths=0.5, cmap=\"vlag\", square=True)\n",
    "plt.title('Cosine Similarity for Acquisition 2 vs 4')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2_vs_v4_xdev.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cossim per devname, no idle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract 300 bins from the kde plot\n",
    "def extract_bins (df,device_mapping,seconds=17.6,microseconds=0):\n",
    "\n",
    "    df['Device Type'] = df['Source IEEE'].map(device_mapping)\n",
    "    \n",
    "    areas_per_device = {}\n",
    "    kde_list = []\n",
    "    # Density plot based on inter-arrival time for each category\n",
    "    colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "    for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "        if device_type == 'Coordinator' or device_type == 'nan' or device_type is None or device_type is np.nan:\n",
    "            continue\n",
    "\n",
    "        subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "        all_timedelta_milli_device = []\n",
    "        \n",
    "        print(f\"Device type: {device_type}\")\n",
    "\n",
    "        for device_address in subset_df['Source IEEE'].unique():\n",
    "            device_df = subset_df[subset_df['Source IEEE'] == device_address]\n",
    "            device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            list_of_timedelta = []\n",
    "            for i in range(1, len(device_df)):\n",
    "                date_string = device_df.loc[i, 'Time']\n",
    "                date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "                list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "            filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=seconds, microseconds=microseconds) and td > timedelta(days=0)]\n",
    "\n",
    "            filtered_timedelta_milli_device = [td.total_seconds()*1000 for td in filtered_timedelta_device]\n",
    "            \n",
    "            \n",
    "            all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "            \n",
    "        \n",
    "        if len(all_timedelta_milli_device) <= 5:\n",
    "            continue\n",
    "        \n",
    "        plt.figure()\n",
    "        kde_plot = sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "        kde_list.append({'device':device_type,'kde':kde_plot})\n",
    "    \n",
    "    glob_min = 0\n",
    "    glob_max = 0\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        # Extract KDE Data:\n",
    "        kde = elem['kde']\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = x_values.min(), x_values.max()\n",
    "        if glob_max < x_max:\n",
    "            glob_max = x_max\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        kde = elem['kde']\n",
    "        # Extract KDE Data:\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = glob_min,glob_max\n",
    "        bin_width = (x_max - x_min) / 300\n",
    "        bins = np.linspace(x_min, x_max, 301)  # 301 points for 300 intervals\n",
    "\n",
    "        # Area Calculation:\n",
    "        areas = []\n",
    "        for i in range(len(bins) - 1):\n",
    "            bin_start, bin_end = bins[i], bins[i + 1]\n",
    "            bin_midpoint = (bin_start + bin_end) / 2\n",
    "\n",
    "            # Use Kernel Density Estimation Function Directly (no need for interpolation):\n",
    "            density = kde_line.get_ydata()[np.argmin(np.abs(x_values - bin_midpoint))]\n",
    "            area = density * bin_width\n",
    "            areas.append(area)\n",
    "\n",
    "        areas_per_device[elem['device']] = areas\n",
    "        \n",
    "    # for device_type, areas in areas_per_device.items():\n",
    "        # plt.figure()\n",
    "        # plt.bar(bins[:-1], areas, width=bin_width, align='edge', edgecolor='k', alpha=0.5)\n",
    "        # plt.xlabel('Time Interval (ms)')\n",
    "        # plt.ylabel('Area')\n",
    "        # plt.title(f'Areas under KDE for {device_type}')\n",
    "        # plt.show()\n",
    "    \n",
    "    return areas_per_device\n",
    "\n",
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hist2 = extract_bins(df2,device_name_mapping,0,100000)\n",
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "hist4 = extract_bins(df4,device_name_mapping,0,100000)\n",
    "hist2 = dict(sorted(hist2.items()))\n",
    "hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "print(hist2.keys())\n",
    "print(hist4.keys())\n",
    "\n",
    "# hist2_clean = hist2.copy()\n",
    "\n",
    "# Filter hist2 to contain only keys that are also in hist4\n",
    "hist2_clean = {key: hist2[key] for key in hist4.keys() if key in hist2}\n",
    "\n",
    "# # Ensure Dictionaries are Sorted for Consistent Order\n",
    "# sorted_hist2 = dict(sorted(hist2_clean.items()))\n",
    "# sorted_hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist4), len(hist2_clean)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2_clean.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "        distance_matrix_total[j, i] =np.dot(hist1_val, hist2_val) / (np.linalg.norm(hist1_val) * np.linalg.norm(hist2_val))\n",
    "\n",
    "print(distance_matrix_total)\n",
    "\n",
    "\n",
    "sns.heatmap(distance_matrix_total, xticklabels=hist2_clean.keys(), yticklabels=hist4.keys(), linewidths=0.5, cmap=\"vlag\", square=True)\n",
    "plt.title('Cosine Similarity for Acquisition 2 vs 4')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2_vs_v4_xdev.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity per Device full, no idle _old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance_matrix_v2 = [[0 for x in range(len(devices_hist_v2))] for y in range(len(devices_hist_v2))]\n",
    "\n",
    "\n",
    "for i in range(len(devices_hist_v2)):\n",
    "    for j in range(len(devices_hist_v2)):\n",
    "        \n",
    "        distance_matrix_v2[i][j] = np.dot(devices_hist_v2[i], devices_hist_v2[j]) / (np.linalg.norm(devices_hist_v2[i]) * np.linalg.norm(devices_hist_v2[j]))\n",
    "\n",
    "\n",
    "sns.heatmap(distance_matrix_v2, xticklabels=devices_name_v2, yticklabels=devices_name_v2, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 2')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix_v4 = [[0 for x in range(len(devices_hist_v4))] for y in range(len(devices_hist_v4))]\n",
    "\n",
    "for i in range(len(devices_hist_v4)):\n",
    "    for j in range(len(devices_hist_v4)):\n",
    "        distance_matrix_v4[i][j] = np.dot(devices_hist_v4[i], devices_hist_v4[j]) / (np.linalg.norm(devices_hist_v4[i]) * np.linalg.norm(devices_hist_v4[j]))\n",
    "\n",
    "sns.heatmap(distance_matrix_v4, xticklabels=devices_name_v4, yticklabels=devices_name_v4, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 4')\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix_tot = [[0 for x in range(len(devices_hist_v2))] for y in range(len(devices_hist_v4))]\n",
    "\n",
    "for i in range(len(devices_hist_v2)):\n",
    "    for j in range(len(devices_hist_v4)):\n",
    "        distance_matrix_tot[i][j] = np.dot(devices_hist_v2[i], devices_hist_v4[j]) / (np.linalg.norm(devices_hist_v2[i]) * np.linalg.norm(devices_hist_v4[j]))\n",
    "\n",
    "sns.heatmap(distance_matrix_tot, xticklabels=devices_name_v2, yticklabels=devices_name_v4, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "\n",
    "#sns.heatmap(distance_matrix_total[0:22, 22:45], xticklabels=devices_name_v4, yticklabels=devices_name_v2, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix in between Acquisition 2 and 4')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2_vs_v4_per_device.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEVENST Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_distance_matrix_v2 = [[0 for x in range(len(devices_hist_v2))] for y in range(len(devices_hist_v2))]\n",
    "\n",
    "for i in range(len(devices_hist_v2)):\n",
    "    for j in range(len(devices_hist_v2)):\n",
    "        lev_distance_matrix_v2[i][j] = lev.distance(str(devices_hist_v2[i]), str(devices_hist_v2[j]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_distance_matrix_v4 = [[0 for x in range(len(devices_hist_v4))] for y in range(len(devices_hist_v4))]\n",
    "\n",
    "for i in range(len(devices_hist_v4)):\n",
    "    for j in range(len(devices_hist_v4)):\n",
    "        lev_distance_matrix_v4[i][j] = lev.distance(str(devices_hist_v4[i]), str(devices_hist_v4[j]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_devices_hist = devices_hist_v2 + devices_hist_v4\n",
    "\n",
    "lev_distance_matrix_total = np.zeros((len(total_devices_hist), len(total_devices_hist)), dtype=int)\n",
    "\n",
    "for i in range(len(total_devices_hist)):\n",
    "    for j in range(len(total_devices_hist)):\n",
    "        lev_distance_matrix_total[i][j] = lev.distance(str(total_devices_hist[i]), str(total_devices_hist[j]))\n",
    "\n",
    "print(lev_distance_matrix_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lev_distance_matrix_total[0:22, 22:45], xticklabels=devices_name_v4, yticklabels=devices_name_v2, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix in between Acquisition 2 and 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lev_distance_matrix_v2, xticklabels=devices_name, yticklabels=devices_name, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lev_distance_matrix_v4, xticklabels=devices_name, yticklabels=devices_name)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "idle_df = df[df['File'].str.contains('idle')]\n",
    "\n",
    "idle_df.head()\n",
    "\n",
    "sns.kdeplot(df['Length'], bw_method=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df['Device Type'] = df['Source Zigbee'].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_micro_device = []\n",
    "\n",
    "    for device_address in subset_df['Source Zigbee'].unique():\n",
    "        device_df = subset_df[subset_df['Source Zigbee'] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=50, microseconds=10000) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "\n",
    "        all_timedelta_micro_device.extend(filtered_timedelta_micro_device)\n",
    "\n",
    "    sns.kdeplot(all_timedelta_micro_device, bw_method=0.1, label=device_type, color=color)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "#save_tikzplotlib(plt.gcf(), \"category_density_updated.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density function for specific device\n",
    "\n",
    "device_address = '0x0a79'\n",
    "\n",
    "device_df = df[(df['Source Zigbee'] == device_address)]\n",
    "device_df = device_df.sort_values(by=['Time'])\n",
    "print(device_address)\n",
    "print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "date_string = device_df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(device_df)):\n",
    "    date_string = device_df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1\n",
    "\n",
    "filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=20, microseconds=0) and td > timedelta(days=0)]\n",
    "print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) / 1e6 for td in filtered_timedelta_device]\n",
    "if len(filtered_timedelta_micro_device) > 0:\n",
    "    print(f\"{max(filtered_timedelta_micro_device)} seconds\")\n",
    "\n",
    "\n",
    "#plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address)\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Seconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f\"Histogram of Timedelta Values\")\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=100, range=(0, 20))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating mean and standard deviation from the total dataframe\n",
    "inter_arrival_time = []\n",
    "df = df.sort_values(by=['Time'])\n",
    "\n",
    "date_string = df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    date_string = df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    iat = date_object1 - date_object\n",
    "    if(iat > timedelta(days=0, hours=0, seconds=0, microseconds=0) and\n",
    "       iat < timedelta(days=0, hours=0, seconds=0, microseconds=150000)):\n",
    "        inter_arrival_time.append(iat)\n",
    "    date_object = date_object1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iat_mean = np.mean(inter_arrival_time)\n",
    "\n",
    "micro_iat = np.array([td.total_seconds() for td in inter_arrival_time])\n",
    "iat_stdev = np.std(micro_iat)\n",
    "\n",
    "print(f\"Mean is {iat_mean}\")\n",
    "print(f\"Standard deviation is {iat_stdev} seconds\")\n",
    "\n",
    "iat_threshold = iat_mean.total_seconds() + 3 * iat_stdev\n",
    "print(f\"Threshold is {iat_threshold} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the time delta between packets from a specific device grouped by destination zigbee\n",
    "\n",
    "device_address = '0x310d'\n",
    "\n",
    "device_flux_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "device_flux_df = device_flux_df.sort_values(by=['Time'])\n",
    "print(f\"The dataframe for device {device_address} is made up by {len(device_flux_df)} packets\")\n",
    "\n",
    "device_flux_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#group by destination zigbee\n",
    "device_flux_df = device_flux_df.groupby('Destination Zigbee')\n",
    "\n",
    "legend_labels = []\n",
    "\n",
    "for destination_address, group_data in device_flux_df:\n",
    "    #if destination_address != device_address:\n",
    "    group = device_flux_df.get_group(destination_address)\n",
    "    group.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    date_string = group.loc[0,'Time']\n",
    "    date_string_without_zeros1 = date_string[:-3]\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    date_object = datetime.strptime(date_string_without_zeros1, date_format)\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(group)):\n",
    "        date_string = group.loc[i,'Time']\n",
    "        date_string_without_zeros = date_string[:-3]\n",
    "        date_object1 = datetime.strptime(date_string_without_zeros, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    threshold = timedelta(days=0, hours=0, seconds=0, microseconds=100000)\n",
    "    filtered = [td for td in list_of_timedelta if td < threshold]\n",
    "    list_micro = [td.microseconds + 1000000 * td.seconds for td in filtered]\n",
    "\n",
    "    if len(list_micro) > 1:\n",
    "        mean = np.mean(list_micro)\n",
    "        std = np.std(list_micro)\n",
    "\n",
    "        print(f\"Mean for destination {destination_address} is: {mean}\")\n",
    "        print(f\"Standard deviation for destination {destination_address} is: {std}\")\n",
    "        print(\"Calculated on {} packets\".format(len(list_micro)))\n",
    "        print(\"\")\n",
    "        legend_labels.append(destination_address)\n",
    "        plt.hist(list_micro, bins=100, edgecolor='k', alpha=0.7, label=destination_address)\n",
    "    \n",
    "    plt.legend(legend_labels, title='Destination Address')\n",
    "    plt.xlabel('Time in Microseconds')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Timedelta Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the time delta between a flux of two specific devices\n",
    "\n",
    "device_address = '0x0000'\n",
    "\n",
    "device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "device_packets = device_packets.sort_values(by=['Time'])\n",
    "device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "communicating_device = device_packets['Destination Zigbee'].unique()\n",
    "mask = communicating_device != device_address\n",
    "communicating_device = communicating_device[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(communicating_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_device = '0xaea8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication = device_packets[(device_packets['Destination Zigbee'] == other_device) | (device_packets['Source Zigbee'] == other_device)]\n",
    "communication.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_string = communication.loc[0,'Time']\n",
    "date_string_without_zeros1 = date_string[:-3]\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string_without_zeros1, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(communication)):\n",
    "    date_string = communication.loc[i,'Time']\n",
    "    date_string_without_zeros = date_string[:-3]\n",
    "    date_object1 = datetime.strptime(date_string_without_zeros, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_of_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=0)\n",
    "max_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=50000)\n",
    "filtered = [td for td in list_of_timedelta if td > min_threshold]\n",
    "filtered = [td for td in filtered if td < max_threshold]\n",
    "list_micro = [td.microseconds + 1000000 * td.seconds for td in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_labels = []\n",
    "\n",
    "if len(list_micro) > 1:\n",
    "    mean = np.mean(list_micro)\n",
    "    std = np.std(list_micro)\n",
    "\n",
    "    print(f\"Mean for destination {other_device} is: {mean}\")\n",
    "    print(f\"Standard deviation for destination {other_device} is: {std}\")\n",
    "    print(\"Calculated on {} packets\".format(len(list_micro)))\n",
    "    print(\"\")\n",
    "\n",
    "plt.hist(list_micro, bins=500, edgecolor='k')\n",
    "legend_labels.append(other_device)\n",
    "plt.legend(legend_labels, title='Device in Communication')\n",
    "plt.xlabel('Time in Microseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values for device {}'.format(device_address))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the dataframe into trains of packets according to the threshold\n",
    "\n",
    "# for each device i will have a list of list: a list of trains and each train is a list of packets\n",
    "# there will be the same train for both the devices in communication\n",
    "# from the packets in the train i will extract the features (delta time, packet length, packet type, payload, fcf, ecc)\n",
    "\n",
    "train_of_packets = []\n",
    "all_trains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device_address in devices:\n",
    "\n",
    "    device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2:\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    trains.to_csv(f'trains_{device_address}.csv', index=False)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    train_of_packets = []\n",
    "    all_trains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bin_edges) - 1):\n",
    "    print(f\"Bin {i + 1}: {bin_edges[i]:.2f} to {bin_edges[i + 1]:.2f} --> Count: {int(hist[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Possible MAC addresses for each device\n",
    "\n",
    "mac_addresses = df.groupby('Source Zigbee')['Extended Source'].unique().reset_index()\n",
    "\n",
    "for source, extended in zip(mac_addresses['Source Zigbee'], mac_addresses['Extended Source']):\n",
    "    if source not in devices:\n",
    "        mac_addresses.drop(mac_addresses[mac_addresses['Source Zigbee'] == source].index, inplace=True)\n",
    "    else:\n",
    "        mac_addresses.loc[mac_addresses['Source Zigbee'] == source, 'Source Zigbee'] = device_name_mapping[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model IAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ---------------------------------------- RANDOM FOREST ON DENSITY FUNCTION ---------------------------------------- ###\n",
    "\n",
    "## Matrix composed by the hystograms of the time delta between packets for each device removing acks and link status\n",
    "def create_ml_dataset_tot(df,threshold=67130,number_of_features=100):\n",
    "    if type(threshold) is not timedelta:\n",
    "        threshold = timedelta(microseconds=threshold)\n",
    "    total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "    train_of_packets = []\n",
    "\n",
    "\n",
    "    for device_address in devices:\n",
    "\n",
    "        \n",
    "        # CREATE THE TRAINS OF PACKETS\n",
    "        all_trains = []\n",
    "        \n",
    "        device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "        device_packets = device_packets.sort_values(by=['Time'])\n",
    "        device_packets = device_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "        device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        prev_timestamp = None\n",
    "        for index, row in device_packets.iterrows():\n",
    "            date_string = row['Time']\n",
    "            date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "            timestamp = datetime.strptime(date_string, date_format)\n",
    "            if prev_timestamp is not None:\n",
    "                delta_time = timestamp - prev_timestamp\n",
    "                if delta_time < threshold:\n",
    "                    train_of_packets.append(row)\n",
    "                else:\n",
    "                    if len(train_of_packets) >= 2 and train_of_packets[0]['Source Zigbee'] == device_address:\n",
    "                        all_trains.append(train_of_packets)\n",
    "                    train_of_packets = [row]\n",
    "\n",
    "            prev_timestamp = timestamp\n",
    "\n",
    "        if len(train_of_packets) >= 2:\n",
    "            all_trains.append(train_of_packets)\n",
    "\n",
    "        trains = pd.DataFrame(all_trains)\n",
    "\n",
    "        del all_trains\n",
    "\n",
    "        # REMOVE DUPLICATES\n",
    "        cleaned_df = []\n",
    "        for i in range(len(trains)):\n",
    "            train = trains.loc[i]\n",
    "            seen_packets = set()\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None:\n",
    "                    packet_tuple = (\n",
    "                        packet['Length'],\n",
    "                        packet['FCF IEEE'],\n",
    "                        packet['Sequence Number'],\n",
    "                        packet['Source Zigbee'],\n",
    "                        packet['Destination Zigbee'],\n",
    "                        packet['Payload Length']\n",
    "                    )\n",
    "                    if packet_tuple not in seen_packets:\n",
    "                        seen_packets.add(packet_tuple)\n",
    "                        result.append(packet)\n",
    "            cleaned_df.append(result)\n",
    "\n",
    "        cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "        # REMOVE ACK PACKETS AND LINK STATUS\n",
    "        no_ack_df = []\n",
    "        for i in range(len(cleaned_df)):\n",
    "            train = cleaned_df.loc[i]\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                    result.append(packet)\n",
    "            no_ack_df.append(result)\n",
    "\n",
    "        no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "        # CREATE THE HISTOGRAMS\n",
    "        histograms = []\n",
    "        for i in range(len(no_ack_df)):\n",
    "            train = no_ack_df.loc[i]\n",
    "            list_of_timedelta = []\n",
    "            for j in range(1, len(train)):\n",
    "                if train[j] is not None:\n",
    "                    date_string = train[j]['Time']\n",
    "                    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                    date_object = datetime.strptime(date_string, date_format)\n",
    "                    date_string1 = train[j-1]['Time']\n",
    "                    date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                    list_of_timedelta.append(date_object - date_object1)\n",
    "            list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "            hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "            histograms.append(hist)\n",
    "\n",
    "        # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "        histogram_matrix = np.array(histograms)\n",
    "        last_column = np.repeat(device_address, len(histogram_matrix))\n",
    "        matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "        # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "        total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n",
    "    \n",
    "    return total_matrix\n",
    "\n",
    "\n",
    "## Matrix composed by the hystograms of the time delta between outgoing packets for each device removing acks and link status\n",
    "\n",
    "def create_ml_dataset_out(df,threshold=67130,number_of_features=100):\n",
    "    if type(threshold) is not timedelta:\n",
    "        threshold = timedelta(microseconds=threshold)\n",
    "    total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "    train_of_packets = []\n",
    "\n",
    "\n",
    "    for device_address in devices:\n",
    "        \n",
    "        # CREATE THE TRAINS OF PACKETS\n",
    "        all_trains = []\n",
    "        \n",
    "        device_packets = df[(df['Source Zigbee'] == device_address)]\n",
    "        device_packets = device_packets.sort_values(by=['Time'])\n",
    "        device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "        device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        prev_timestamp = None\n",
    "        for index, row in device_packets.iterrows():\n",
    "            date_string = row['Time']\n",
    "            date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "            timestamp = datetime.strptime(date_string, date_format)\n",
    "            if prev_timestamp is not None:\n",
    "                delta_time = timestamp - prev_timestamp\n",
    "                if delta_time < threshold:\n",
    "                    train_of_packets.append(row)\n",
    "                else:\n",
    "                    if len(train_of_packets) >= 2 or device_address == '0x907b':\n",
    "                        all_trains.append(train_of_packets)\n",
    "                    train_of_packets = [row]\n",
    "\n",
    "            prev_timestamp = timestamp\n",
    "\n",
    "        if len(train_of_packets) >= 2:\n",
    "            all_trains.append(train_of_packets)\n",
    "\n",
    "        trains = pd.DataFrame(all_trains)\n",
    "\n",
    "        del all_trains\n",
    "\n",
    "        # REMOVE DUPLICATES\n",
    "        cleaned_df = []\n",
    "        for i in range(len(trains)):\n",
    "            train = trains.loc[i]\n",
    "            seen_packets = set()\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None:\n",
    "                    packet_tuple = (\n",
    "                        packet['Length'],\n",
    "                        packet['FCF IEEE'],\n",
    "                        packet['Sequence Number'],\n",
    "                        packet['Source Zigbee'],\n",
    "                        packet['Destination Zigbee'],\n",
    "                        packet['Payload Length']\n",
    "                    )\n",
    "                    if packet_tuple not in seen_packets:\n",
    "                        seen_packets.add(packet_tuple)\n",
    "                        result.append(packet)\n",
    "            cleaned_df.append(result)\n",
    "\n",
    "        cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "        # REMOVE ACK PACKETS AND LINK STATUS\n",
    "        no_ack_df = []\n",
    "        for i in range(len(cleaned_df)):\n",
    "            train = cleaned_df.loc[i]\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                    result.append(packet)\n",
    "            no_ack_df.append(result)\n",
    "\n",
    "        no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "        # CREATE THE HISTOGRAMS\n",
    "        histograms = []\n",
    "        for i in range(len(no_ack_df)):\n",
    "            train = no_ack_df.loc[i]\n",
    "            list_of_timedelta = []\n",
    "            for j in range(1, len(train)):\n",
    "                if train[j] is not None:\n",
    "                    date_string = train[j]['Time']\n",
    "                    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                    date_object = datetime.strptime(date_string, date_format)\n",
    "                    date_string1 = train[j-1]['Time']\n",
    "                    date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                    list_of_timedelta.append(date_object - date_object1)\n",
    "            list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "            hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "            histograms.append(hist)\n",
    "\n",
    "        # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "        histogram_matrix = np.array(histograms)\n",
    "        last_column = np.repeat(device_address, len(histogram_matrix))\n",
    "        matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "        # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "        total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n",
    "    return total_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev classificatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = True #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=17.6, microseconds=0)\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "\n",
    "tot_mtrx2 = create_ml_dataset_out(df2,threshold,30)\n",
    "\n",
    "\n",
    "last_column = pd.Series(tot_mtrx2[:, -1])\n",
    "\n",
    "if CLS:           \n",
    "    last_column_mapped = last_column.map(device_type_mapping)\n",
    "else:\n",
    "    last_column_mapped = last_column.map(device_name_mapping)\n",
    "\n",
    "tot_mtrx2[:, -1] = last_column_mapped.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = True #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=17.6, microseconds=0)\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "tot_mtrx4 =  create_ml_dataset_out(df4,threshold,30)\n",
    "\n",
    "\n",
    "last_column = pd.Series(tot_mtrx4[:, -1])\n",
    "\n",
    "if CLS:           \n",
    "    last_column_mapped = last_column.map(device_type_mapping)\n",
    "else:\n",
    "    last_column_mapped = last_column.map(device_name_mapping)\n",
    "\n",
    "\n",
    "tot_mtrx4[:, -1] = last_column_mapped.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "X_train = tot_mtrx2[:, :-1]\n",
    "y_train = tot_mtrx2[:, -1]\n",
    "\n",
    "X_test = tot_mtrx4[:, :-1]\n",
    "y_test = tot_mtrx4[:, -1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Classifier on the total matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "X = tot_mtrx2[:, :-1]\n",
    "y = tot_mtrx2[:, -1]\n",
    "\n",
    "# X_test = tot_mtrx4[:, :-1]\n",
    "# y_test = tot_mtrx4[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = False #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "ID_SIMP = False #If True it will be used the simple mapping for device identification\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=100000)\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "\n",
    "\n",
    "tot_mtrx2 = create_ml_dataset_out(df2,threshold,30)\n",
    "\n",
    "\n",
    "last_column = pd.Series(tot_mtrx2[:, -1])\n",
    "\n",
    "if CLS:\n",
    "    last_column_mapped = last_column.map(device_type_mapping)\n",
    "    \n",
    "else:\n",
    "    if ID_SIMP:\n",
    "        last_column_mapped = last_column.map(device_name_mapping_simple)\n",
    "    else:\n",
    "        last_column_mapped = last_column.map(device_name_mapping)\n",
    "\n",
    "tot_mtrx2[:, -1] = last_column_mapped.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = False #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "ID_SIMP = False #If True it will be used the simple mapping for device identification\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=100000)\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "\n",
    "tot_mtrx4 = create_ml_dataset_out(df4,threshold,30)\n",
    "\n",
    "last_column = pd.Series(tot_mtrx4[:, -1])\n",
    "\n",
    "if CLS:           \n",
    "    last_column_mapped = last_column.map(device_type_mapping)\n",
    "else:\n",
    "    if ID_SIMP:\n",
    "        last_column_mapped = last_column.map(device_name_mapping_simple)\n",
    "    else:\n",
    "        last_column_mapped = last_column.map(device_name_mapping)\n",
    "    \n",
    "\n",
    "tot_mtrx4[:, -1] = last_column_mapped.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "X_train = tot_mtrx2[:, :-1]\n",
    "y_train = tot_mtrx2[:, -1]\n",
    "\n",
    "X_test = tot_mtrx4[:, :-1]\n",
    "y_test = tot_mtrx4[:, -1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Classifier on the total matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = tot_mtrx2[:, :-1]\n",
    "y = tot_mtrx2[:, -1]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "max_depths = range(15, 25)\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "\n",
    "# for max_depth in max_depths:\n",
    "#     clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth)\n",
    "#     f1score = []\n",
    "#     accuracy = []\n",
    "#     for train_index, test_index in cv.split(X, y):\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index] \n",
    "#         clf.fit(X_train, y_train)\n",
    "#         y_pred = clf.predict(X_test)\n",
    "#         #f1score.append(f1_score(y_test, y_pred, average='macro'))\n",
    "#         accuracy.append(accuracy_score(y_test, y_pred))\n",
    "#     #f1_scores.append(np.mean(f1score))\n",
    "#     accuracies.append(np.mean(accuracy))\n",
    "\n",
    "# plt.plot(max_depths, accuracies)\n",
    "# plt.xlabel('Max Depth')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy vs Max Depth')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X = tot_mtrx2[:, :-1]\n",
    "y = tot_mtrx2[:, -1]\n",
    "\n",
    "# X_test = tot_mtrx4[:, :-1]\n",
    "# y_test = tot_mtrx4[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix composed by the hystograms of the time delta between outgoing packets for each device removing acks and link status\n",
    "\n",
    "# number_of_features = 100\n",
    "# total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "# train_of_packets = []\n",
    "\n",
    "\n",
    "# for device_address in devices:\n",
    "    \n",
    "#     # CREATE THE TRAINS OF PACKETS\n",
    "#     all_trains = []\n",
    "    \n",
    "#     device_packets = df[(df['Source Zigbee'] == device_address)]\n",
    "#     device_packets = device_packets.sort_values(by=['Time'])\n",
    "#     device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "#     device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#     prev_timestamp = None\n",
    "#     for index, row in device_packets.iterrows():\n",
    "#         date_string = row['Time']\n",
    "#         date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "#         timestamp = datetime.strptime(date_string, date_format)\n",
    "#         if prev_timestamp is not None:\n",
    "#             delta_time = timestamp - prev_timestamp\n",
    "#             if delta_time < threshold:\n",
    "#                 train_of_packets.append(row)\n",
    "#             else:\n",
    "#                 if len(train_of_packets) >= 2 or device_address == '0x907b':\n",
    "#                     all_trains.append(train_of_packets)\n",
    "#                 train_of_packets = [row]\n",
    "\n",
    "#         prev_timestamp = timestamp\n",
    "\n",
    "#     if len(train_of_packets) >= 2:\n",
    "#         all_trains.append(train_of_packets)\n",
    "\n",
    "#     trains = pd.DataFrame(all_trains)\n",
    "\n",
    "#     del all_trains\n",
    "\n",
    "#     # REMOVE DUPLICATES\n",
    "#     cleaned_df = []\n",
    "#     for i in range(len(trains)):\n",
    "#         train = trains.loc[i]\n",
    "#         seen_packets = set()\n",
    "#         result = []\n",
    "#         for packet in train:\n",
    "#             if packet is not None:\n",
    "#                 packet_tuple = (\n",
    "#                     packet['Length'],\n",
    "#                     packet['FCF IEEE'],\n",
    "#                     packet['Sequence Number'],\n",
    "#                     packet['Source Zigbee'],\n",
    "#                     packet['Destination Zigbee'],\n",
    "#                     packet['Payload Length']\n",
    "#                 )\n",
    "#                 if packet_tuple not in seen_packets:\n",
    "#                     seen_packets.add(packet_tuple)\n",
    "#                     result.append(packet)\n",
    "#         cleaned_df.append(result)\n",
    "\n",
    "#     cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "#     # REMOVE ACK PACKETS AND LINK STATUS\n",
    "#     no_ack_df = []\n",
    "#     for i in range(len(cleaned_df)):\n",
    "#         train = cleaned_df.loc[i]\n",
    "#         result = []\n",
    "#         for packet in train:\n",
    "#             if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "#                 result.append(packet)\n",
    "#         no_ack_df.append(result)\n",
    "\n",
    "#     no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "#     # CREATE THE HISTOGRAMS\n",
    "#     histograms = []\n",
    "#     for i in range(len(no_ack_df)):\n",
    "#         train = no_ack_df.loc[i]\n",
    "#         list_of_timedelta = []\n",
    "#         for j in range(1, len(train)):\n",
    "#             if train[j] is not None:\n",
    "#                 date_string = train[j]['Time']\n",
    "#                 date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "#                 date_object = datetime.strptime(date_string, date_format)\n",
    "#                 date_string1 = train[j-1]['Time']\n",
    "#                 date_object1 = datetime.strptime(date_string1, date_format)\n",
    "#                 list_of_timedelta.append(date_object - date_object1)\n",
    "#         list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "#         hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "#         histograms.append(hist)\n",
    "\n",
    "#     # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "#     histogram_matrix = np.array(histograms)\n",
    "#     last_column = np.repeat(device_address, len(histogram_matrix))\n",
    "#     matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "#     # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "#     total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Matrix composed by the hystograms of the time delta between outgoing packets for each type of device removing acks and link status\n",
    "\n",
    "# number_of_features = 30\n",
    "# total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "# train_of_packets = []\n",
    "\n",
    "\n",
    "# for device_address in devices:\n",
    "    \n",
    "#     # CREATE THE TRAINS OF PACKETS\n",
    "#     all_trains = []\n",
    "    \n",
    "#     device_packets = df[(df['Source Zigbee'] == device_address)]\n",
    "#     device_packets = device_packets.sort_values(by=['Time'])\n",
    "#     device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "#     device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#     prev_timestamp = None\n",
    "#     for index, row in device_packets.iterrows():\n",
    "#         date_string = row['Time']\n",
    "#         date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "#         timestamp = datetime.strptime(date_string, date_format)\n",
    "#         if prev_timestamp is not None:\n",
    "#             delta_time = timestamp - prev_timestamp\n",
    "#             if delta_time < threshold:\n",
    "#                 train_of_packets.append(row)\n",
    "#             else:\n",
    "#                 if len(train_of_packets) >= 2 or device_address == '0x907b':\n",
    "#                     all_trains.append(train_of_packets)\n",
    "#                 train_of_packets = [row]\n",
    "\n",
    "#         prev_timestamp = timestamp\n",
    "\n",
    "#     if len(train_of_packets) >= 2:\n",
    "#         all_trains.append(train_of_packets)\n",
    "\n",
    "#     trains = pd.DataFrame(all_trains)\n",
    "\n",
    "#     del all_trains\n",
    "\n",
    "#     # REMOVE DUPLICATES\n",
    "#     cleaned_df = []\n",
    "#     for i in range(len(trains)):\n",
    "#         train = trains.loc[i]\n",
    "#         seen_packets = set()\n",
    "#         result = []\n",
    "#         for packet in train:\n",
    "#             if packet is not None:\n",
    "#                 packet_tuple = (\n",
    "#                     packet['Length'],\n",
    "#                     packet['FCF IEEE'],\n",
    "#                     packet['Sequence Number'],\n",
    "#                     packet['Source Zigbee'],\n",
    "#                     packet['Destination Zigbee'],\n",
    "#                     packet['Payload Length']\n",
    "#                 )\n",
    "#                 if packet_tuple not in seen_packets:\n",
    "#                     seen_packets.add(packet_tuple)\n",
    "#                     result.append(packet)\n",
    "#         cleaned_df.append(result)\n",
    "\n",
    "#     cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "#     # REMOVE ACK PACKETS AND LINK STATUS\n",
    "#     no_ack_df = []\n",
    "#     for i in range(len(cleaned_df)):\n",
    "#         train = cleaned_df.loc[i]\n",
    "#         result = []\n",
    "#         for packet in train:\n",
    "#             if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "#                 result.append(packet)\n",
    "#         no_ack_df.append(result)\n",
    "\n",
    "#     no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "#     # CREATE THE HISTOGRAMS\n",
    "#     histograms = []\n",
    "#     for i in range(len(no_ack_df)):\n",
    "#         train = no_ack_df.loc[i]\n",
    "#         list_of_timedelta = []\n",
    "#         for j in range(1, len(train)):\n",
    "#             if train[j] is not None:\n",
    "#                 date_string = train[j]['Time']\n",
    "#                 date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "#                 date_object = datetime.strptime(date_string, date_format)\n",
    "#                 date_string1 = train[j-1]['Time']\n",
    "#                 date_object1 = datetime.strptime(date_string1, date_format)\n",
    "#                 list_of_timedelta.append(date_object - date_object1)\n",
    "#         list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "#         hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "#         histograms.append(hist)\n",
    "\n",
    "#     # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "#     histogram_matrix = np.array(histograms)\n",
    "#     last_column = np.repeat(device_type_mapping[device_address], len(histogram_matrix))\n",
    "#     matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "#     # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "#     total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsfres_df (df,threshold):\n",
    "    if threshold is None:\n",
    "        threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "    \n",
    "    # Dataframe with train of packets without acks and link status\n",
    "\n",
    "    train_of_packets = []\n",
    "    no_ack_df = []\n",
    "\n",
    "    for device_address in devices:\n",
    "        \n",
    "        # CREATE THE TRAINS OF PACKETS\n",
    "        all_trains = []\n",
    "        \n",
    "        device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "        device_packets = device_packets.sort_values(by=['Time'])\n",
    "        device_packets = device_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "        device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        prev_timestamp = None\n",
    "        for index, row in device_packets.iterrows():\n",
    "            date_string = row['Time']\n",
    "            date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "            timestamp = datetime.strptime(date_string, date_format)\n",
    "            if prev_timestamp is not None:\n",
    "                delta_time = timestamp - prev_timestamp\n",
    "                if delta_time < threshold:\n",
    "                    train_of_packets.append(row)\n",
    "                else:\n",
    "                    if len(train_of_packets) >= 2:\n",
    "                        all_trains.append(train_of_packets)\n",
    "                    train_of_packets = [row]\n",
    "\n",
    "            prev_timestamp = timestamp\n",
    "\n",
    "        if len(train_of_packets) >= 2:\n",
    "            all_trains.append(train_of_packets)\n",
    "\n",
    "        trains = pd.DataFrame(all_trains)\n",
    "\n",
    "        del all_trains\n",
    "\n",
    "        # REMOVE DUPLICATES\n",
    "        cleaned_df = []\n",
    "        for i in range(len(trains)):\n",
    "            train = trains.loc[i]\n",
    "            seen_packets = set()\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None:\n",
    "                    packet_tuple = (\n",
    "                        packet['Length'],\n",
    "                        packet['FCF IEEE'],\n",
    "                        packet['Sequence Number'],\n",
    "                        packet['Source Zigbee'],\n",
    "                        packet['Destination Zigbee'],\n",
    "                        packet['Payload Length']\n",
    "                    )\n",
    "                    if packet_tuple not in seen_packets:\n",
    "                        seen_packets.add(packet_tuple)\n",
    "                        result.append(packet)\n",
    "            cleaned_df.append(result)\n",
    "\n",
    "        cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "        # REMOVE ACK PACKETS AND LINK STATUS\n",
    "        for i in range(len(cleaned_df)):\n",
    "            train = cleaned_df.loc[i]\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                    result.append(packet)\n",
    "            if(len(result) > 0 and result[0]['Source Zigbee'] in devices):\n",
    "                no_ack_df.append(result)\n",
    "\n",
    "    no_ack_df = pd.DataFrame(no_ack_df)\n",
    "    \n",
    "    # Create a single DataFrame with all the packets and 'id' column to identify the train of packets\n",
    "\n",
    "    all_packets = list()\n",
    "\n",
    "    for i in range(len(no_ack_df)):\n",
    "        train = no_ack_df.loc[i]\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                all_packets.append({\n",
    "                        'id': i,\n",
    "                        'Time': packet['Time'],\n",
    "                        'Delta Time': packet['Delta Time'],\n",
    "                        'Length': packet['Length'],\n",
    "                        'Sequence Number': packet['Sequence Number'],\n",
    "                        'Payload Length': packet['Payload Length'] if not np.isnan(packet['Payload Length']) else 0})\n",
    "\n",
    "    result_df = pd.DataFrame(all_packets)\n",
    "\n",
    "    # Set Delta Time of the first packet of each train to 0\n",
    "    id_changes = result_df['id'] != result_df['id'].shift(1)\n",
    "    result_df.loc[id_changes, 'Delta Time'] = 0\n",
    "    \n",
    "\n",
    "    ## Tsfresh used to get features from trains of packets\n",
    "\n",
    "    from tsfresh import extract_features\n",
    "    from tsfresh.utilities.dataframe_functions import impute\n",
    "    from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters, ComprehensiveFCParameters\n",
    "\n",
    "    result_df['Time'] = pd.to_datetime(result_df['Time'], format=\"%b %d, %Y %H:%M:%S.%f\")\n",
    "\n",
    "    # Extract features from the dataframe\n",
    "    extracted_features = extract_features(result_df, column_id=\"id\", column_sort=\"Time\", default_fc_parameters=MinimalFCParameters())\n",
    "\n",
    "    # Impute the extracted features\n",
    "    impute(extracted_features)\n",
    "\n",
    "    # Add the device address to the dataframe of features to be able to classify the packets\n",
    "    for i in range(len(no_ack_df)):\n",
    "        extracted_features.loc[i, 'Source Zigbee'] = no_ack_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    \n",
    "    \n",
    "\n",
    "    # Adding to every row of extracted_features the number of packets that fall inside the windows of threshold / 10\n",
    "\n",
    "    # N = 10\n",
    "    # for i in range(len(extracted_features)):\n",
    "    #     mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    #     sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    #     deltas = result_df[result_df['id'] == i]['Delta Time']\n",
    "    #     for n in range(N):\n",
    "    #         lower_bound = n * (threshold_float / N)\n",
    "    #         upper_bound = (n + 1) * (threshold_float / N)\n",
    "    #         count = ((deltas >= lower_bound) & \n",
    "    #                 (deltas < upper_bound)).sum()\n",
    "\n",
    "    #         # Add the count to the extracted features\n",
    "    #         match n:\n",
    "    #             case 0:\n",
    "    #                 extracted_features.loc[i, 'Window 1'] = count\n",
    "    #             case 1:\n",
    "    #                 extracted_features.loc[i, 'Window 2'] = count\n",
    "    #             case 2:\n",
    "    #                 extracted_features.loc[i, 'Window 3'] = count\n",
    "    #             case 3:\n",
    "    #                 extracted_features.loc[i, 'Window 4'] = count\n",
    "    #             case 4:\n",
    "    #                 extracted_features.loc[i, 'Window 5'] = count\n",
    "    #             case 5:\n",
    "    #                 extracted_features.loc[i, 'Window 6'] = count\n",
    "    #             case 6:\n",
    "    #                 extracted_features.loc[i, 'Window 7'] = count\n",
    "    #             case 7:\n",
    "    #                 extracted_features.loc[i, 'Window 8'] = count\n",
    "    #             case 8:\n",
    "    #                 extracted_features.loc[i, 'Window 9'] = count\n",
    "    #             case 9:\n",
    "    #                 extracted_features.loc[i, 'Window 10'] = count\n",
    "    #             case _:\n",
    "    #                 print('Error')    \n",
    "    \n",
    "\n",
    "\n",
    "    return extracted_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def rf_eval (train_df,test_df=None,topk=50):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    if test_df is None:\n",
    "        # Assuming `tfresh_mtrx2` is your DataFrame\n",
    "        X = train_df.drop(columns=['Source Zigbee'])\n",
    "        y = train_df['Source Zigbee']\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        X = train_df.drop(columns=['Source Zigbee'])\n",
    "        X_train = train_df.drop(columns=['Source Zigbee'])\n",
    "        y_train = train_df['Source Zigbee']\n",
    "\n",
    "        X_test = test_df.drop(columns=['Source Zigbee'])\n",
    "        y_test = test_df['Source Zigbee']\n",
    "    \n",
    "\n",
    "    # Instantiate the RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the RandomForestClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the RandomForestClassifier on the testing data\n",
    "    print('Features Importance')\n",
    "    feature_importances = clf.feature_importances_\n",
    "    print(feature_importances)\n",
    "    print('')\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro') \n",
    "    class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "\n",
    "    # Get feature importances and their corresponding feature names\n",
    "    feature_importances = clf.feature_importances_\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importances_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importances\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by importance\n",
    "    feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "    # Select the top 50 features\n",
    "    top_50_features = feature_importances_df.head(topk)\n",
    "\n",
    "    # # Plot the top 50 features\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # plt.barh(top_50_features['feature'], top_50_features['importance'])\n",
    "    # plt.xlabel('Feature Importance')\n",
    "    # plt.ylabel('Feature Name')\n",
    "    # plt.title('Top 50 Feature Importances')\n",
    "    # plt.gca().invert_yaxis()\n",
    "    # plt.show()\n",
    "\n",
    "    # Select only the top 50 features for training a new model\n",
    "    top_50_feature_names = top_50_features['feature'].values\n",
    "    X_train_top50 = X_train[top_50_feature_names]\n",
    "    X_test_top50 = X_test[top_50_feature_names]\n",
    "\n",
    "    # Train a new RandomForestClassifier with the top 50 features\n",
    "    clf_top50 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf_top50.fit(X_train_top50, y_train)\n",
    "\n",
    "    # Evaluate the new RandomForestClassifier on the testing data\n",
    "    y_pred_top50 = clf_top50.predict(X_test_top50)\n",
    "\n",
    "    accuracy_top50 = accuracy_score(y_test, y_pred_top50)\n",
    "    precision_top50 = precision_score(y_test, y_pred_top50, average='macro', zero_division=0)\n",
    "    recall_top50 = recall_score(y_test, y_pred_top50, average='macro')\n",
    "    f1_top50 = f1_score(y_test, y_pred_top50, average='macro')\n",
    "\n",
    "    print(f\"Accuracy (Top 50 features): {accuracy_top50}\")\n",
    "    print(f\"Precision (Top 50 features): {precision_top50}\")\n",
    "    print(f\"Recall (Top 50 features): {recall_top50}\")\n",
    "    print(f\"F1 (Top 50 features): {f1_top50}\")\n",
    "\n",
    "    print(top_50_features)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def select_top_k_features(X_train, y_train, k):\n",
    "    # Train a RandomForestClassifier to get feature importances\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importances and their corresponding feature names\n",
    "    feature_importances = clf.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importances_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importances\n",
    "    })\n",
    "    \n",
    "    # Sort the DataFrame by importance\n",
    "    feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    # Select the top k features\n",
    "    top_k_features = feature_importances_df.head(k)\n",
    "    \n",
    "    # Plot the top k features\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # plt.barh(top_k_features['feature'], top_k_features['importance'])\n",
    "    # plt.xlabel('Feature Importance')\n",
    "    # plt.ylabel('Feature Name')\n",
    "    # plt.title(f'Top {k} Feature Importances')\n",
    "    # plt.gca().invert_yaxis()\n",
    "    # plt.show()\n",
    "    \n",
    "    # Return the names of the top k features\n",
    "    return top_k_features['feature'].values\n",
    "\n",
    "\n",
    "def train_and_evaluate_xgboost(X_train, X_test, y_train, y_test):\n",
    "    # Encode the class labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    \n",
    "    xgb_model = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "    xgb_model.fit(X_train, y_train_encoded)\n",
    "    \n",
    "    y_pred_encoded = xgb_model.predict(X_test)\n",
    "    y_pred = le.inverse_transform(y_pred_encoded)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    \n",
    "    # Optionally, display the confusion matrix\n",
    "    # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=le.classes_)\n",
    "    # disp.plot(cmap=plt.cm.Blues)\n",
    "    # plt.title('Confusion Matrix for XGBoost')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def xgb_eval (train_df,test_df=None,topk=50):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    if test_df is None:\n",
    "        # Assuming `tfresh_mtrx2` is your DataFrame\n",
    "        X = train_df.drop(columns=['Source Zigbee'])\n",
    "        y = train_df['Source Zigbee']\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        X_train = train_df.drop(columns=['Source Zigbee'])\n",
    "        y_train = train_df['Source Zigbee']\n",
    "\n",
    "        X_test = test_df.drop(columns=['Source Zigbee'])\n",
    "        y_test = test_df['Source Zigbee']\n",
    "\n",
    "    # Select the top 50 features\n",
    "    top_50_feature_names = select_top_k_features(X_train, y_train, topk)\n",
    "\n",
    "    # Create datasets with the top 50 features\n",
    "    X_train_top50 = X_train[top_50_feature_names]\n",
    "    X_test_top50 = X_test[top_50_feature_names]\n",
    "\n",
    "    # Train and evaluate XGBoost on the full feature set\n",
    "    print(\"Evaluating XGBoost on the full feature set:\")\n",
    "    train_and_evaluate_xgboost(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Train and evaluate XGBoost on the top 50 feature set\n",
    "    print(\"Evaluating XGBoost on the top 50 feature set:\")\n",
    "    train_and_evaluate_xgboost(X_train_top50, X_test_top50, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n"
     ]
    }
   ],
   "source": [
    "#see if file tfresh_mtrx2_min is present\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if os.path.isfile('tfresh_mtrx2_min.pkl'):\n",
    "    print('File exists')\n",
    "    with open('tfresh_mtrx2_min.pkl', 'rb') as file:\n",
    "        tfresh_mtrx2 = pickle.load(file)\n",
    "\n",
    "else:\n",
    "\n",
    "    VERSION = 2 #4\n",
    "    IDLE_FILTER = False\n",
    "\n",
    "    CLS = True #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "    df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "    load_mapping(VERSION)\n",
    "    devices_name_v2 = devices_name.copy()\n",
    "\n",
    "    if IDLE_FILTER:\n",
    "        df2 = df2[df2['File'].str.contains('idle')]\n",
    "        df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    threshold = timedelta(days=0, hours=0, seconds=17.6, microseconds=0)\n",
    "    threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "\n",
    "    tfresh_mtrx2 = tsfres_df(df2,threshold)\n",
    "\n",
    "\n",
    "    # last_column = pd.Series(tot_mtrx2[:, -1])\n",
    "\n",
    "    # if CLS:           \n",
    "    #     last_column_mapped = last_column.map(device_type_mapping)\n",
    "    # else:\n",
    "    #     last_column_mapped = last_column.map(device_name_mapping)\n",
    "\n",
    "    # tot_mtrx2[:, -1] = last_column_mapped.values\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    # Define your variable\n",
    "\n",
    "    # Specify the file path where you want to save the pickle file\n",
    "    pickle_file_path = 'tfresh_mtrx2_min.pkl'\n",
    "\n",
    "    # Open the file in write-binary mode and save the variable\n",
    "    with open(pickle_file_path, 'wb') as file:\n",
    "        pickle.dump(tfresh_mtrx2, file)\n",
    "\n",
    "    print(f'Variable \"test\" has been saved to {pickle_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if os.path.isfile('tfresh_mtrx4_min.pkl'):\n",
    "    print('File exists')\n",
    "    with open('tfresh_mtrx4_min.pkl', 'rb') as file:\n",
    "        tfresh_mtrx2 = pickle.load(file)\n",
    "\n",
    "else:\n",
    "\n",
    "    #load df from csv\n",
    "    VERSION = 4\n",
    "    IDLE_FILTER = False\n",
    "\n",
    "    CLS = True #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "    df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "    load_mapping(VERSION)\n",
    "    devices_name_v4 = devices_name.copy()\n",
    "\n",
    "    if IDLE_FILTER:\n",
    "        df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "    threshold = timedelta(days=0, hours=0, seconds=17.6, microseconds=0)\n",
    "    threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "\n",
    "    tfresh_mtrx4 = tsfres_df(df4,threshold)\n",
    "\n",
    "\n",
    "    # last_column = pd.Series(tot_mtrx4[:, -1])\n",
    "\n",
    "    # if CLS:           \n",
    "    #     last_column_mapped = last_column.map(device_type_mapping)\n",
    "    # else:\n",
    "    #     last_column_mapped = last_column.map(device_name_mapping)\n",
    "\n",
    "\n",
    "    # tot_mtrx4[:, -1] = last_column_mapped.values\n",
    "\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    # Define your variable\n",
    "\n",
    "    # Specify the file path where you want to save the pickle file\n",
    "    pickle_file_path = 'tfresh_mtrx4_min.pkl'\n",
    "\n",
    "    # Open the file in write-binary mode and save the variable\n",
    "    with open(pickle_file_path, 'wb') as file:\n",
    "        pickle.dump(tfresh_mtrx4, file)\n",
    "\n",
    "    print(f'Variable \"test\" has been saved to {pickle_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MINI = True\n",
    "CLS = False #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "if MINI:\n",
    "    tfresh_mtrx4 = pd.read_pickle('tfresh_mtrx4_min.pkl')\n",
    "    tfresh_mtrx2 = pd.read_pickle('tfresh_mtrx2_min.pkl')\n",
    "else:\n",
    "    tfresh_mtrx4 = pd.read_pickle('tfresh_mtrx4.pkl')\n",
    "    tfresh_mtrx2 = pd.read_pickle('tfresh_mtrx2.pkl')\n",
    "\n",
    "\n",
    "if CLS:\n",
    "    load_mapping(4)     \n",
    "    tfresh_mtrx4['Source Zigbee'] = tfresh_mtrx4['Source Zigbee'].map(device_type_mapping)\n",
    "    load_mapping(2)\n",
    "    tfresh_mtrx2['Source Zigbee'] = tfresh_mtrx2['Source Zigbee'].map(device_type_mapping)\n",
    "else: \n",
    "    load_mapping(4)\n",
    "    tfresh_mtrx4['Source Zigbee'] = tfresh_mtrx4['Source Zigbee'].map(device_name_mapping_simple)\n",
    "    load_mapping(2)\n",
    "    tfresh_mtrx2['Source Zigbee'] = tfresh_mtrx2['Source Zigbee'].map(device_name_mapping_simple)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MiniDataset:{MINI}, Device Classification Task?:{CLS}\")\n",
    "\n",
    "# print(\"Baseline RF\")\n",
    "# rf_eval(tfresh_mtrx2,tfresh_mtrx2,50)\n",
    "# print(\"Results RF\")\n",
    "# rf_eval(tfresh_mtrx2,tfresh_mtrx4,50)\n",
    "\n",
    "print(\"Baseline XGB\")\n",
    "xgb_eval(tfresh_mtrx2,tfresh_mtrx2,50)\n",
    "print(\"Results XGB\")\n",
    "xgb_eval(tfresh_mtrx2,tfresh_mtrx4,50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unusde Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Assuming `tfresh_mtrx2` is your DataFrame\n",
    "X = tfresh_mtrx2.drop(columns=['Source Zigbee'])\n",
    "y = tfresh_mtrx2['Source Zigbee']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "feature_importances = clf.feature_importances_\n",
    "print(feature_importances)\n",
    "print('')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# Get feature importances and their corresponding feature names\n",
    "feature_importances = clf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances_df = feature_importances_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Select the top 50 features\n",
    "top_50_features = feature_importances_df.head(50)\n",
    "\n",
    "# # Plot the top 50 features\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.barh(top_50_features['feature'], top_50_features['importance'])\n",
    "# plt.xlabel('Feature Importance')\n",
    "# plt.ylabel('Feature Name')\n",
    "# plt.title('Top 50 Feature Importances')\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.show()\n",
    "\n",
    "# Select only the top 50 features for training a new model\n",
    "top_50_feature_names = top_50_features['feature'].values\n",
    "X_train_top50 = X_train[top_50_feature_names]\n",
    "X_test_top50 = X_test[top_50_feature_names]\n",
    "\n",
    "# Train a new RandomForestClassifier with the top 50 features\n",
    "clf_top50 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_top50.fit(X_train_top50, y_train)\n",
    "\n",
    "# Evaluate the new RandomForestClassifier on the testing data\n",
    "y_pred_top50 = clf_top50.predict(X_test_top50)\n",
    "\n",
    "accuracy_top50 = accuracy_score(y_test, y_pred_top50)\n",
    "precision_top50 = precision_score(y_test, y_pred_top50, average='macro', zero_division=0)\n",
    "recall_top50 = recall_score(y_test, y_pred_top50, average='macro')\n",
    "f1_top50 = f1_score(y_test, y_pred_top50, average='macro')\n",
    "\n",
    "print(f\"Accuracy (Top 50 features): {accuracy_top50}\")\n",
    "print(f\"Precision (Top 50 features): {precision_top50}\")\n",
    "print(f\"Recall (Top 50 features): {recall_top50}\")\n",
    "print(f\"F1 (Top 50 features): {f1_top50}\")\n",
    "\n",
    "print(top_50_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost classifiers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "X_train = X_train_top50\n",
    "X_test = X_test_top50\n",
    "y_train = tfresh_mtrx2['Source Zigbee']\n",
    "y_test = tfresh_mtrx4['Source Zigbee']\n",
    "\n",
    "# Instantiate the XGBoostClassifier\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=22, random_state=42)\n",
    "\n",
    "# Train the XGBoostClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the XGBoostClassifier on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X = tfresh_mtrx2.drop(columns=['Source Zigbee'])\n",
    "y = tfresh_mtrx2['Source Zigbee']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# #plot_confusion_matrix(y_test, y_pred)\n",
    "# cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=devices)\n",
    "# disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "# plt.show()\n",
    "# #plt.figure(figsize=(10, 10))\n",
    "# #sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "# #plt.xlabel('Predicted')\n",
    "# #plt.ylabel('Actual')\n",
    "# #plt.title('Confusion Matrix')\n",
    "# #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "X = tfresh_mtrx2.drop(columns=['Source Zigbee'])\n",
    "y = tfresh_mtrx2['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the RandomForestClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the features with low importance\n",
    "feature_threshold = 0.00000\n",
    "selected_features = np.where(clf.feature_importances_ > feature_threshold)[0]\n",
    "selected_features = X_train.columns[selected_features]\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to every row of extracted_features the number of packets that fall inside the window {μ-2σ, μ-2σ + i(4σ/N)}\n",
    "\n",
    "N = 5\n",
    "max_batches = 0\n",
    "id_max_batches = 0\n",
    "\n",
    "for i in range(len(extracted_features)):\n",
    "    batches = 0\n",
    "    mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    maximum = extracted_features.loc[i, 'Delta Time__maximum']\n",
    "\n",
    "    print(f\"Batches for train {i}\")\n",
    "    print(f\"Mean: {mu}, std dev: {sigma}\")\n",
    "    print(f\"Maximum: {maximum}\")\n",
    "    if(sigma == 0):\n",
    "        continue\n",
    "    for n in range(0,10):\n",
    "        batch = mu - 2 * sigma + n * (4 * sigma / N)\n",
    "        if(batch < 0):\n",
    "            continue\n",
    "        if(batch > maximum):\n",
    "            break\n",
    "        batches += 1\n",
    "        print(batch)\n",
    "    if(batches > max_batches):\n",
    "        max_batches = batches\n",
    "        id_max_batches = i\n",
    "    print('')\n",
    "\n",
    "print(f\"Max batches: {max_batches} for train {id_max_batches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(N):\n",
    "    lower_bound = n * (threshold_float / N)\n",
    "    upper_bound = (n + 1) * (threshold_float / N)\n",
    "    print(f\"{lower_bound} -> {upper_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = result_df.loc[12694,'Delta Time']\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a / threshold_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x_points = np.linspace(mu - 2 * sigma, mu + 2 * sigma, 100)\n",
    "y_points = norm.pdf(x_points, mu, sigma)\n",
    "\n",
    "plt.plot(x_points, y_points, color='black', label=f\"Mean: {mu:.2f}, Std Dev: {sigma:.2f}\")\n",
    "plt.title('Graph of the Probability Density Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost classifiers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(y))}\n",
    "target = y.map(class_mapping)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2)\n",
    "\n",
    "# Instantiate the XGBoostClassifier\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=22, random_state=42)\n",
    "\n",
    "# Train the XGBoostClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the XGBoostClassifier on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBClassifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'extracted_features' is your DataFrame\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "target = y.map(class_mapping)\n",
    "\n",
    "# Instantiate the XGBoostClassifier\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=22, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, target):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Train the XGBoostClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM DEEP Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- COORDINATOR DATAFRAME -------------------------------------------------- ###\n",
    "\n",
    "# Dataframe with train of packets without acks and link status\n",
    "def generate_coord_df (df, devices, threshold):\n",
    "\n",
    "    train_of_packets = []\n",
    "    coordinator_final_df = []\n",
    "\n",
    "    device_address = '0x0000'\n",
    "        \n",
    "    # CREATE THE TRAINS OF PACKETS\n",
    "    all_trains = []\n",
    "\n",
    "    coordinator_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    coordinator_packets = coordinator_packets.sort_values(by=['Time'])\n",
    "    coordinator_packets = coordinator_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee','FCF Zigbee', 'Payload Length', 'Action']]\n",
    "    coordinator_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in coordinator_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2:\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    coordinator_trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    # REMOVE DUPLICATES\n",
    "    coordinator_cleaned_df = []\n",
    "    for i in range(len(coordinator_trains)):\n",
    "        train = coordinator_trains.loc[i]\n",
    "        seen_packets = set()\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                packet_tuple = (\n",
    "                    packet['Length'],\n",
    "                    packet['FCF IEEE'],\n",
    "                    packet['Sequence Number'],\n",
    "                    packet['Source Zigbee'],\n",
    "                    packet['Destination Zigbee'],\n",
    "                    packet['Payload Length']\n",
    "                    )\n",
    "                if packet_tuple not in seen_packets:\n",
    "                    seen_packets.add(packet_tuple)\n",
    "                    result.append(packet)\n",
    "        coordinator_cleaned_df.append(result)\n",
    "\n",
    "    coordinator_cleaned_df = pd.DataFrame(coordinator_cleaned_df)\n",
    "\n",
    "    # REMOVE LINK STATUS\n",
    "    for i in range(len(coordinator_cleaned_df)):\n",
    "        train = coordinator_cleaned_df.loc[i]\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None and packet['FCF IEEE'] != \"0x8841\":\n",
    "                result.append(packet)\n",
    "        if(len(result) > 0 and result[0]['Source Zigbee'] in devices):\n",
    "            coordinator_final_df.append(result)\n",
    "\n",
    "    coordinator_final_df = pd.DataFrame(coordinator_final_df)\n",
    "    return coordinator_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ++++++++++++++ MATRIX FOR BIDIRECTIONAL NEURAL NETWORKS ++++++++++++++ ###\n",
    "def matrix_for_bilstm (coordinator_final_df, device_name_mapping):\n",
    "    \n",
    "    # Counts the number of packets for each train and make mean\n",
    "    not_null = coordinator_final_df.notnull().sum(axis=1)\n",
    "    mean_not_null = not_null.mean()\n",
    "    rounded_mean = math.ceil(mean_not_null)\n",
    "\n",
    "    num_of_features = 6\n",
    "\n",
    "    # Create a train matrix for bidirectional neural networks with the following features for first 14 packets in a train:\n",
    "    # Delta Time, Length, Payload Length, Source Zigbee, FCF Zigbee, IAT from last ingoing or outgoing packet\n",
    "    matrix_for_binn = np.zeros((len(coordinator_final_df), rounded_mean, num_of_features))\n",
    "\n",
    "    for i in range(len(coordinator_final_df)):\n",
    "        train = coordinator_final_df.loc[i]\n",
    "        j = 0\n",
    "        sum_delta_1 = 0\n",
    "        sum_delta_0 = 0\n",
    "        seen_1 = False\n",
    "        seen_0 = False\n",
    "        for packet in train:\n",
    "            if j < rounded_mean and packet is not None:\n",
    "                matrix_for_binn[i][j][0] = packet['Delta Time']\n",
    "                matrix_for_binn[i][j][1] = packet['Length']\n",
    "                matrix_for_binn[i][j][2] = packet['Payload Length']\n",
    "                matrix_for_binn[i][j][3] = 1 if packet['Source Zigbee'] == '0x0000' else 0 # 1 if coordinator, 0 if not\n",
    "                matrix_for_binn[i][j][4] = 0 if pd.isna(packet['FCF Zigbee']) else 1 # 1 if zigbee, 0 if not\n",
    "\n",
    "                # iat from last ingoing or outgoing packet\n",
    "                if matrix_for_binn[i][j][3] == 1:\n",
    "                    if seen_1:\n",
    "                        sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "                        sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "                        matrix_for_binn[i][j][5] = sum_delta_1\n",
    "                        sum_delta_1 = 0\n",
    "                    else:\n",
    "                        matrix_for_binn[i][j][5] = 0\n",
    "                        seen_1 = True\n",
    "                        if seen_0:\n",
    "                            sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "                else:\n",
    "                    if seen_0:\n",
    "                        sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "                        sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "                        matrix_for_binn[i][j][5] = sum_delta_0\n",
    "                        sum_delta_0 = 0\n",
    "                    else:\n",
    "                        matrix_for_binn[i][j][5] = 0\n",
    "                        seen_0 = True\n",
    "                        if seen_1:\n",
    "                            sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "\n",
    "                j += 1\n",
    "\n",
    "\n",
    "    # List to keep track of the device address of each train\n",
    "    device_address_list = []\n",
    "\n",
    "    for i in range(len(coordinator_final_df)):\n",
    "        train = coordinator_final_df.loc[i]\n",
    "        device_address_list.append(device_name_mapping[train[0]['Source Zigbee']])\n",
    "    \n",
    "    return rounded_mean,num_of_features, matrix_for_binn, device_address_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_for_bilstm_test (coordinator_final_df, device_name_mapping,rounded_mean,num_of_features):\n",
    "    # Create test matrix for bidirectional neural networks with same features as train matrix\n",
    "    test_matrix = np.zeros((len(coordinator_final_df), rounded_mean, num_of_features))\n",
    "\n",
    "    for i in range(len(coordinator_final_df)):\n",
    "        train = coordinator_final_df.loc[i]\n",
    "        j = 0\n",
    "        for packet in train:\n",
    "            if j < rounded_mean and packet is not None:\n",
    "                test_matrix[i][j][0] = packet['Delta Time']\n",
    "                test_matrix[i][j][1] = packet['Length']\n",
    "                test_matrix[i][j][2] = packet['Payload Length']\n",
    "                test_matrix[i][j][3] = 1 if packet['Source Zigbee'] == '0x0000' else 0 # 1 if coordinator, 0 if not\n",
    "                test_matrix[i][j][4] = 0 if pd.isna(packet['FCF Zigbee']) else 1 # 1 if zigbee, 0 if not\n",
    "\n",
    "                # iat from last ingoing or outgoing packet\n",
    "                if test_matrix[i][j][3] == 1:\n",
    "                    if seen_1:\n",
    "                        sum_delta_0 += test_matrix[i][j][0]\n",
    "                        sum_delta_1 += test_matrix[i][j][0]\n",
    "                        test_matrix[i][j][5] = sum_delta_1\n",
    "                        sum_delta_1 = 0\n",
    "                    else:\n",
    "                        test_matrix[i][j][5] = 0\n",
    "                        seen_1 = True\n",
    "                        if seen_0:\n",
    "                            sum_delta_0 += test_matrix[i][j][0]\n",
    "                else:\n",
    "                    if seen_0:\n",
    "                        sum_delta_0 += test_matrix[i][j][0]\n",
    "                        sum_delta_1 += test_matrix[i][j][0]\n",
    "                        test_matrix[i][j][5] = sum_delta_0\n",
    "                        sum_delta_0 = 0\n",
    "                    else:\n",
    "                        test_matrix[i][j][5] = 0\n",
    "                        seen_0 = True\n",
    "                        if seen_1:\n",
    "                            sum_delta_1 += test_matrix[i][j][0]\n",
    "                j += 1\n",
    "\n",
    "\n",
    "    test_device_address_list = []\n",
    "\n",
    "    for i in range(len(coordinator_final_df)):\n",
    "        train = coordinator_final_df.loc[i]\n",
    "        test_device_address_list.append(device_name_mapping[train[0]['Source Zigbee']])\n",
    "    \n",
    "    return test_matrix, test_device_address_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_vlidate(model, matrix_for_binn, device_address_list, test_matrix, test_device_address_list):\n",
    "    import tensorflow as tf\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.utils import resample\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "    from tensorflow.keras import backend as K\n",
    "    from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "    # Encode the device address\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(device_address_list)\n",
    "    encoded_Y = encoder.transform(device_address_list)\n",
    "    encoded_Y_test = encoder.transform(test_device_address_list)\n",
    "\n",
    "\n",
    "    # Select features to use\n",
    "    features_to_use = [0, 1, 2, 3, 4]\n",
    "    num_of_features = len(features_to_use)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(matrix_for_binn[:, :, features_to_use], encoded_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # X_train = matrix_for_binn[:, :, features_to_use]\n",
    "    # y_train = encoded_Y\n",
    "    X_val = test_matrix[:, :, features_to_use]\n",
    "    y_val = encoded_Y_test\n",
    "\n",
    "        # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # Predict the labels for the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate and print the F1 score\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    return  X_val, y_val\n",
    "\n",
    "\n",
    "def train_bilstm_model(matrix_for_binn, device_address_list, test_matrix, test_device_address_list,rounded_mean,number_of_epochs):\n",
    "# Train and testing on different dataset\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.utils import resample\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "    from tensorflow.keras import backend as K\n",
    "    from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "    # Encode the device address\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(device_address_list)\n",
    "    encoded_Y = encoder.transform(device_address_list)\n",
    "    encoded_Y_test = encoder.transform(test_device_address_list)\n",
    "\n",
    "\n",
    "    # Select features to use\n",
    "    features_to_use = [0, 1, 2, 3, 4]\n",
    "    num_of_features = len(features_to_use)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(matrix_for_binn[:, :, features_to_use], encoded_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # X_train = matrix_for_binn[:, :, features_to_use]\n",
    "    # y_train = encoded_Y\n",
    "    X_val = test_matrix[:, :, features_to_use]\n",
    "    y_val = encoded_Y_test\n",
    "\n",
    "\n",
    "    sampling = True\n",
    "\n",
    "    if sampling:\n",
    "        # Convert device_address_list to numpy array for easy indexing\n",
    "        labels = np.array(encoded_Y)\n",
    "\n",
    "        # Identify the unique class labels\n",
    "        unique_classes = np.unique(labels)\n",
    "\n",
    "        # Initialize empty lists to store the balanced dataset\n",
    "        balanced_features = []\n",
    "        balanced_labels = []\n",
    "\n",
    "        # Set the desired number of samples per class\n",
    "        samples_per_class = 200\n",
    "\n",
    "        # Iterate over each class and perform class-wise sampling\n",
    "        for class_label in unique_classes:\n",
    "            # Get indices of instances belonging to the current class\n",
    "            class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "            # Check if the class has enough samples for sampling\n",
    "            if len(class_indices) >= samples_per_class:\n",
    "                # Sample a fixed number of instances from the current class\n",
    "                sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "                # Append the sampled instances to the balanced dataset\n",
    "                balanced_features.extend(matrix_for_binn[sampled_indices][:, :, features_to_use])\n",
    "                balanced_labels.extend(labels[sampled_indices])\n",
    "            else:\n",
    "                # If the class has fewer than 50 samples, include all of them\n",
    "                balanced_features.extend(matrix_for_binn[class_indices][:, :, features_to_use])\n",
    "                balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "        # Convert the balanced dataset to numpy arrays\n",
    "        balanced_features = np.array(balanced_features)\n",
    "        balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "        X_train = balanced_features\n",
    "        y_train = balanced_labels\n",
    "\n",
    "    # Custom F1ScoreCallback\n",
    "    class F1ScoreCallback(Callback):\n",
    "        def __init__(self, validation_data=()):\n",
    "            super(Callback, self).__init__()\n",
    "            self.validation_data = validation_data\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if self.validation_data is not None:\n",
    "                x_val, y_val = self.validation_data\n",
    "                y_pred = self.model.predict(x_val)\n",
    "                y_pred = tf.argmax(y_pred, axis=1)\n",
    "                \n",
    "                f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "                print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    #model.add(Bidirectional(LSTM(16)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Instantiate F1ScoreCallback\n",
    "    f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "    # Instantiate ModelCheckpoint callback\n",
    "    checkpoint_filepath = 'best_model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=number_of_epochs, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "    # Load the best model\n",
    "    print('Loading best model...')\n",
    "    model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # Predict the labels for the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate and print the F1 score\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    \n",
    "    return checkpoint_filepath, model, encoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_bilstm_balanced(matrix_for_binn, device_address_list, rounded_mean, samples_per_class=50, num_epochs=50, test_size=0.2, random_state=42):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from sklearn.utils import resample\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import f1_score\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "    from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "    # Encode the device address\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(device_address_list)\n",
    "    encoded_Y = encoder.transform(device_address_list)\n",
    "\n",
    "    # Convert device_address_list to numpy array for easy indexing\n",
    "    labels = np.array(encoded_Y)\n",
    "\n",
    "    # Identify the unique class labels\n",
    "    unique_classes = np.unique(labels)\n",
    "\n",
    "    # Initialize empty lists to store the balanced dataset\n",
    "    balanced_features = []\n",
    "    balanced_labels = []\n",
    "\n",
    "    # Iterate over each class and perform class-wise sampling\n",
    "    for class_label in unique_classes:\n",
    "        # Get indices of instances belonging to the current class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "        # Check if the class has enough samples for sampling\n",
    "        if len(class_indices) >= samples_per_class:\n",
    "            # Sample a fixed number of instances from the current class\n",
    "            sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=random_state)\n",
    "\n",
    "            # Append the sampled instances to the balanced dataset\n",
    "            balanced_features.extend(matrix_for_binn[sampled_indices])\n",
    "            balanced_labels.extend(labels[sampled_indices])\n",
    "        else:\n",
    "            # If the class has fewer than the desired samples, include all of them\n",
    "            balanced_features.extend(matrix_for_binn[class_indices])\n",
    "            balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "    # Convert the balanced dataset to numpy arrays\n",
    "    balanced_features = np.array(balanced_features)\n",
    "    balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "        # Select features to use\n",
    "    features_to_use = [0, 1, 2, 3, 4]\n",
    "    num_of_features = len(features_to_use)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Split the balanced dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(balanced_features[:, :, features_to_use], balanced_labels, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Custom F1ScoreCallback\n",
    "    class F1ScoreCallback(Callback):\n",
    "        def __init__(self, validation_data=()):\n",
    "            super(F1ScoreCallback, self).__init__()\n",
    "            self.validation_data = validation_data\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if self.validation_data is not None:\n",
    "                x_val, y_val = self.validation_data\n",
    "                y_pred = self.model.predict(x_val)\n",
    "                y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "                f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "                print(f'F1 Score: {f1}')\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(unique_classes), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Instantiate F1ScoreCallback\n",
    "    f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "    # Instantiate ModelCheckpoint callback\n",
    "    checkpoint_filepath = 'best_model_balanced.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "    # Save the best model\n",
    "    model.save(checkpoint_filepath)\n",
    "    print(f\"Model saved to {checkpoint_filepath}\")\n",
    "\n",
    "    # Load the best model\n",
    "    model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "    print(f\"Model loaded from {checkpoint_filepath}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Predict the labels for the validation set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate and print the F1 score\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    return model, encoder, checkpoint_filepath\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 2\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = False #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#device_name_mapping_simple\n",
    "# threshold = timedelta(days=0, hours=0, seconds=17.6, microseconds=0)\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "\n",
    "coordinator_final_df = generate_coord_df(df2, devices, threshold)\n",
    "\n",
    "rounded_mean,num_of_features, matrix_for_binn, device_address_list = matrix_for_bilstm(coordinator_final_df, device_name_mapping_simple)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "occurrences = Counter(device_address_list)\n",
    "# for occurrence in occurrences:\n",
    "#     print(f\"{occurrence}: {occurrences[occurrence]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 4 #2\n",
    "IDLE_FILTER = False\n",
    "CLS = False #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "# threshold = timedelta(days=0, hours=0, seconds=17.6, microseconds=0)\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=88000)\n",
    "\n",
    "coordinator_final_df = generate_coord_df(df4, devices, threshold)\n",
    "\n",
    "rounded_mean_test,num_of_features_test, matrix_for_binn_test, device_address_list_test = matrix_for_bilstm(coordinator_final_df, device_name_mapping_simple)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "occurrences_test = Counter(device_address_list_test)\n",
    "# for occurrence in occurrences:\n",
    "#     print(f\"{occurrence}: {occurrences[occurrence]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{rounded_mean_test},{num_of_features_test}')\n",
    "print(f'{rounded_mean},{num_of_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath, model_1, encoder=train_bilstm_model(matrix_for_binn, device_address_list, matrix_for_binn_test, device_address_list_test,rounded_mean,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath_2, model_2, encoder_2= train_and_evaluate_bilstm_balanced(matrix_for_binn, device_address_list, rounded_mean, samples_per_class=50, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = model_vlidate(  model_1,matrix_for_binn, device_address_list, matrix_for_binn_test, device_address_list_test)\n",
    "\n",
    "model_1.save('model1_100it_bilstm.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/antonio/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/antonio/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "        Aqara Button       1.00      0.42      0.59       120\n",
      "          Aqara Door       0.79      0.46      0.58       129\n",
      "        Aqara Motion       1.00      1.00      1.00       190\n",
      "     Aqara Vibration       1.00      0.31      0.48        51\n",
      "         Coordinator       1.00      1.00      1.00      2270\n",
      "       Ledvance Bulb       0.83      1.00      0.91       258\n",
      "Ledvance Smart+ Plug       0.44      1.00      0.62        56\n",
      "    Ledvance Z3 Plug       0.00      0.00      0.00        54\n",
      "           Moes Bulb       0.02      1.00      0.03        11\n",
      "        Philips Lamp       0.51      0.95      0.66       296\n",
      "      Philips Motion       0.00      0.00      0.00       203\n",
      "          Power Plug       0.93      0.60      0.73       758\n",
      "        Smart Socket       0.50      0.21      0.29       617\n",
      "         Sonoff Door       0.00      0.00      0.00       112\n",
      "       Sonoff Motion       0.00      0.00      0.00        67\n",
      "  Sonoff Temperature       0.80      1.00      0.89       142\n",
      "\n",
      "            accuracy                           0.73      5334\n",
      "           macro avg       0.55      0.56      0.49      5334\n",
      "        weighted avg       0.80      0.73      0.74      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model = tf.keras.models.load_model('model1_100it_bilstm.keras')\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "consensus = True\n",
    "occurrence_threshold = 35\n",
    "if consensus:\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "\n",
    "decoded_y_test = encoder.inverse_transform(y_test)\n",
    "decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "classes = sorted(set(device_address_list_test))\n",
    "\n",
    "print(classification_report(decoded_y_test, decoded_y_pred_classes, target_names=classes))\n",
    "\n",
    "cm = confusion_matrix(decoded_y_test, decoded_y_pred_classes, normalize='true', labels=classes)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#plt.title('Confusion Matrix first 4 features with sampling')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, f\"{cm[i, j]:.2f}\", ha='center', va='center', color='white' if cm[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"identification_consensus_v2.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7020699275067004\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.669852418833107\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7079909759152977\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.6986215518060026\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7120785062348274\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7129260844634182\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7068521912038392\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7228857986737544\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.710208455900314\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7185498568893801\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7147745421908953\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7260369689297337\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7157952331398971\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7086246030699171\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7165825325948831\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7133162449565605\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7301825706638744\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "0.7127553287042753\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7346107890843674\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7251472805966753\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7194532643676\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7265930764522517\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7246801218189436\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7197015383585151\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "0.7278926986699122\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7206221470763371\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7360702291622646\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "0.7352437043756294\n",
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "0.7323007122962774\n"
     ]
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Function to calculate F1 score for given occurrence_threshold\n",
    "def calculate_f1_score(occurrence_threshold):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "    # Inverse transform labels\n",
    "    decoded_y_test = encoder.inverse_transform(y_test)\n",
    "    decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(decoded_y_test, decoded_y_pred_classes, average='weighted')\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Vary occurrence_threshold values\n",
    "occurrence_thresholds = range(1, 30)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in occurrence_thresholds:\n",
    "    f1 = calculate_f1_score(threshold)\n",
    "    f1_scores.append(f1)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = True\n",
    "if smooth:\n",
    "    f1_interp = interp1d(occurrence_thresholds, f1_scores, kind='cubic')\n",
    "    new_thresholds = np.linspace(min(occurrence_thresholds), max(occurrence_thresholds), 100)\n",
    "    smooth_f1_scores = f1_interp(new_thresholds)\n",
    "    plt.plot(new_thresholds, smooth_f1_scores, marker='', linestyle='-')\n",
    "else:\n",
    "    plt.plot(occurrence_thresholds, f1_scores, marker='.', linestyle='-')\n",
    "\n",
    "plt.xlabel('Occurrence Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Effect of Occurrence Threshold on F1 Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"asyntotic_consensus_cl.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(encoder.inverse_transform(encoded_Y), return_counts=True)\n",
    "value_counts = dict(zip(unique, counts))\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming 'labels' is your array, DataFrame, or list of class labels\n",
    "if isinstance(device_address_list, (np.ndarray, pd.Series)):\n",
    "    class_distribution = np.unique(device_address_list, return_counts=True)\n",
    "else:\n",
    "    class_distribution = Counter(device_address_list)\n",
    "\n",
    "plt.bar(class_distribution.keys(), class_distribution.values())\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional Neural Network with balanced dataset\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert device_address_list to numpy array for easy indexing\n",
    "labels = np.array(encoded_Y)\n",
    "\n",
    "# Identify the unique class labels\n",
    "unique_classes = np.unique(labels)\n",
    "\n",
    "# Initialize empty lists to store the balanced dataset\n",
    "balanced_features = []\n",
    "balanced_labels = []\n",
    "\n",
    "# Set the desired number of samples per class\n",
    "samples_per_class = 50\n",
    "\n",
    "# Iterate over each class and perform class-wise sampling\n",
    "for class_label in unique_classes:\n",
    "    # Get indices of instances belonging to the current class\n",
    "    class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "    # Check if the class has enough samples for sampling\n",
    "    if len(class_indices) >= samples_per_class:\n",
    "        # Sample a fixed number of instances from the current class\n",
    "        sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "        # Append the sampled instances to the balanced dataset\n",
    "        balanced_features.extend(matrix_for_binn[sampled_indices])\n",
    "        balanced_labels.extend(labels[sampled_indices])\n",
    "    else:\n",
    "        # If the class has fewer than 50 samples, include all of them\n",
    "        balanced_features.extend(matrix_for_binn[class_indices])\n",
    "        balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "# Convert the balanced dataset to numpy arrays\n",
    "balanced_features = np.array(balanced_features)\n",
    "balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "# Split the balanced dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_features, balanced_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[f1_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single DataFrame with all the packets and 'id' column to identify the train of packets\n",
    "\n",
    "all_packets = list()\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    for packet in train:\n",
    "        if packet is not None:\n",
    "            all_packets.append({\n",
    "                    'id': i,\n",
    "                    'Time': packet['Time'],\n",
    "                    'Delta Time': packet['Delta Time'],\n",
    "                    'Length': packet['Length'],\n",
    "                    'Sequence Number': packet['Sequence Number'],\n",
    "                    'Payload Length': packet['Payload Length'] if not np.isnan(packet['Payload Length']) else 0,\n",
    "                    'FCF Zigbee': int(packet['FCF Zigbee'], 0),\n",
    "                    'FCF IEEE': int(packet['FCF IEEE'], 0),\n",
    "                    'Action': packet['Action']})\n",
    "\n",
    "coordinator_result_df = pd.DataFrame(all_packets)\n",
    "\n",
    "# Set Delta Time of the first packet of each train to 0\n",
    "id_changes = coordinator_result_df['id'] != coordinator_result_df['id'].shift(1)\n",
    "coordinator_result_df.loc[id_changes, 'Delta Time'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non so cosa sia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Encode the device address\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(device_address_list)\n",
    "encoded_Y = encoder.transform(device_address_list)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrix_for_binn, encoded_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted') # weighted average gives higher f1 score (0.84), macro gives 0.56\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Instantiate ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "# Load the best model\n",
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tsfresh used to get features from trains of packets\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters, ComprehensiveFCParameters\n",
    "\n",
    "coordinator_result_df['Time'] = pd.to_datetime(coordinator_result_df['Time'], format=\"%b %d, %Y %H:%M:%S.%f\")\n",
    "\n",
    "# Remove the 'Action' column for the feature extraction\n",
    "column_to_exclude = 'Action'\n",
    "excluded_column = coordinator_result_df.pop(column_to_exclude)\n",
    "\n",
    "# Extract features from the dataframe\n",
    "extracted_features = extract_features(coordinator_result_df, column_id=\"id\", column_sort=\"Time\", default_fc_parameters=MinimalFCParameters())\n",
    "\n",
    "# Impute the extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Reinsert the 'Action' column\n",
    "coordinator_result_df = pd.concat([coordinator_result_df, excluded_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the device address to the dataframe of features to be able to classify the packets\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    extracted_features.loc[i, 'Source Zigbee'] = device_name_mapping[coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']]\n",
    "    # TODO insert action based on relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    if len(source_action_list) > 1:\n",
    "        print(f\"{i}) {device_name_mapping[source]}: {source_action_list} -> {highest_priority_action}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '0x3d95'\n",
    "source_action_list = ['Report Attributes (0x0a)', 'APS: Ack']\n",
    "print(device_name_mapping[source])\n",
    "\n",
    "highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "print(highest_priority_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# Set the occurrence threshold for label-wise consensus\n",
    "occurrence_threshold = 5\n",
    "occurrences_used = False\n",
    "\n",
    "# Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "for label in np.unique(y_test):\n",
    "    indices = np.where(y_test == label)[0]\n",
    "\n",
    "    # Check if the label has occurred at least occurrence_threshold times\n",
    "    if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "        \n",
    "        # Identify the most common prediction within each occurrence_threshold group\n",
    "        for i in range(0, len(indices), occurrence_threshold):\n",
    "            group_indices = indices[i:i+occurrence_threshold]\n",
    "            most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "            most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[group_indices] = most_frequent_label\n",
    "        \n",
    "    else:\n",
    "        most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "        most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "        # Replace all predictions for this label within the group with the most common value\n",
    "        y_pred[indices] = most_frequent_label\n",
    "\n",
    "\n",
    "# Calculate and store metrics for each fold after label-wise consensus\n",
    "accuracy_consensus = accuracy_score(y_test, y_pred)\n",
    "precision_consensus = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall_consensus = recall_score(y_test, y_pred, average='macro')\n",
    "f1_consensus = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('')\n",
    "print(f\"Accuracy: {accuracy_consensus}\")\n",
    "print(f\"Precision: {precision_consensus}\")\n",
    "print(f\"Recall: {recall_consensus}\")\n",
    "print(f\"F1: {f1_consensus}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "unique_mapped_labels = sorted(set(device_name_mapping.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_mapped_labels)\n",
    "disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "accuracy_scores_consensus = []\n",
    "precision_scores_consensus = []\n",
    "recall_scores_consensus = []\n",
    "f1_scores_consensus = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the RandomForestClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # Set the occurrence threshold for label-wise consensus\n",
    "    occurrence_threshold = 10\n",
    "    occurrences_used = True\n",
    "\n",
    "    # Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "            \n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "                most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred[group_indices] = most_frequent_label\n",
    "        else:\n",
    "            most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "            most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[indices] = most_frequent_label\n",
    "\n",
    "    # Calculate and store metrics for each fold after label-wise consensus\n",
    "    accuracy_scores_consensus.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores_consensus.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores_consensus.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores_consensus.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Average Metrics Across Folds after Label-wise Consensus:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores_consensus) / len(accuracy_scores_consensus)}\")\n",
    "print(f\"Precision: {sum(precision_scores_consensus) / len(precision_scores_consensus)}\")\n",
    "print(f\"Recall: {sum(recall_scores_consensus) / len(recall_scores_consensus)}\")\n",
    "print(f\"F1: {sum(f1_scores_consensus) / len(f1_scores_consensus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score before and after label-wise consensus for different occurrence thresholds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store F1 scores before and after consensus for different occurrence thresholds\n",
    "f1_scores_before_consensus = []\n",
    "f1_scores_after_consensus = []\n",
    "\n",
    "# Define a range of occurrence thresholds from 1 to 20\n",
    "occurrence_threshold_range = range(1, 21)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for occurrence_threshold in occurrence_threshold_range:\n",
    "    # Lists to store F1 scores for each fold\n",
    "    f1_scores = []\n",
    "    f1_scores_consensus = []\n",
    "\n",
    "    for train_index, test_index in stratified_kfold.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the RandomForestClassifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate F1 score before consensus\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "        # Perform label-wise consensus\n",
    "        for label in np.unique(y_test):\n",
    "            indices = np.where(y_test == label)[0]\n",
    "\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "                most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred[group_indices] = most_frequent_label\n",
    "\n",
    "        # Calculate F1 score after consensus\n",
    "        f1_scores_consensus.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # Store average F1 scores for the current occurrence threshold\n",
    "    f1_scores_before_consensus.append(sum(f1_scores) / len(f1_scores))\n",
    "    f1_scores_after_consensus.append(sum(f1_scores_consensus) / len(f1_scores_consensus))\n",
    "\n",
    "# Plot the F1 scores before and after consensus\n",
    "plt.plot(occurrence_threshold_range, f1_scores_before_consensus, label='Before Consensus')\n",
    "plt.plot(occurrence_threshold_range, f1_scores_after_consensus, label='After Consensus')\n",
    "plt.xlabel('Occurrence Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score Before and After Label-wise Consensus')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to every row of extracted_features the number of packets that fall inside the windows of threshold / 10\n",
    "\n",
    "N = 10\n",
    "for i in range(len(extracted_features)):\n",
    "    mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    deltas = coordinator_result_df[coordinator_result_df['id'] == i]['Delta Time']\n",
    "    for n in range(N):\n",
    "        lower_bound = n * (threshold_float / N)\n",
    "        upper_bound = (n + 1) * (threshold_float / N)\n",
    "        count = ((deltas >= lower_bound) & \n",
    "                 (deltas < upper_bound)).sum()\n",
    "\n",
    "        # Add the count to the extracted features\n",
    "        match n:\n",
    "            case 0:\n",
    "                extracted_features.loc[i, 'Window 1'] = count\n",
    "            case 1:\n",
    "                extracted_features.loc[i, 'Window 2'] = count\n",
    "            case 2:\n",
    "                extracted_features.loc[i, 'Window 3'] = count\n",
    "            case 3:\n",
    "                extracted_features.loc[i, 'Window 4'] = count\n",
    "            case 4:\n",
    "                extracted_features.loc[i, 'Window 5'] = count\n",
    "            case 5:\n",
    "                extracted_features.loc[i, 'Window 6'] = count\n",
    "            case 6:\n",
    "                extracted_features.loc[i, 'Window 7'] = count\n",
    "            case 7:\n",
    "                extracted_features.loc[i, 'Window 8'] = count\n",
    "            case 8:\n",
    "                extracted_features.loc[i, 'Window 9'] = count\n",
    "            case 9:\n",
    "                extracted_features.loc[i, 'Window 10'] = count\n",
    "            case _:\n",
    "                print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering on window counts\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "window_features = extracted_features.drop(columns=['Source Zigbee'])\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=16).fit(window_features)\n",
    "clustering.labels_\n",
    "\n",
    "# Adding the cluster labels to the extracted features\n",
    "extracted_features['Cluster'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Window 1', y='Window 2', hue='Cluster', data=extracted_features, palette='viridis', legend='full')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Window 1')\n",
    "plt.ylabel('Window 2')\n",
    "plt.title('Hierarchical Clustering with Source Zigbee Information')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Cluster')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pairwise relationships between the features\n",
    "features_to_plot = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                       'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10', 'Cluster']]\n",
    "\n",
    "# Create a pairplot\n",
    "sns.pairplot(features_to_plot, hue='Cluster', palette='viridis')\n",
    "plt.suptitle('Pairplot of Hierarchical Clustering with Source Zigbee Information', y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between windows and average of windows\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "window_features = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                      'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10']]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(window_features)\n",
    "\n",
    "# Calculate the average cosine similarity for each window\n",
    "average_cosine_similarity = np.mean(cosine_sim, axis=1)\n",
    "\n",
    "# Add the average cosine similarity as a new feature\n",
    "extracted_features['Average Cosine Similarity'] = average_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between windows and array of 1s\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Select features for cosine similarity calculation\n",
    "window_features = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                      'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10']]\n",
    "\n",
    "# Create a vector full of 1s\n",
    "vector_of_ones = np.ones((1, window_features.shape[1]))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim_with_ones = cosine_similarity(window_features, vector_of_ones)\n",
    "\n",
    "# Add the cosine similarity with diagonal as a new feature\n",
    "extracted_features['Cosine Similarity With Diagonal'] = cosine_sim_with_ones[:, 0]  # Assuming you want to use the first column of the cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_feature_importances = sorted(zip(X.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Range of top features to explore (e.g., from 1 to the total number of features)\n",
    "num_features_to_explore = range(1, len(sorted_feature_importances) + 1)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {'num_features': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "# Perform k-fold cross-validation for each number of top features\n",
    "for num_features in num_features_to_explore:\n",
    "    print(f\"Trying with {num_features} top features...\")\n",
    "\n",
    "    # Choose the top N features\n",
    "    top_n_features = [feature for feature, importance in sorted_feature_importances[:num_features]]\n",
    "\n",
    "    # Subset the data with the top features\n",
    "    X_selected = X[top_n_features]\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    y_pred = cross_val_predict(clf, X_selected, y, cv=stratified_kfold)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y, y_pred, average='macro')\n",
    "    f1 = f1_score(y, y_pred, average='macro')\n",
    "\n",
    "    # Store results\n",
    "    results['num_features'].append(num_features)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['precision'].append(precision)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    \n",
    "\n",
    "# Print results\n",
    "for i in range(len(results['num_features'])):\n",
    "    print(f\"Num Features: {results['num_features'][i]}, Accuracy: {results['accuracy'][i]}, Precision: {results['precision'][i]}, Recall: {results['recall'][i]}, F1: {results['f1'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(results['num_features'], results['accuracy'], label='Accuracy', marker='.')\n",
    "\n",
    "# Plot precision\n",
    "plt.plot(results['num_features'], results['precision'], label='Precision', marker='.')\n",
    "\n",
    "# Plot recall\n",
    "plt.plot(results['num_features'], results['recall'], label='Recall', marker='.')\n",
    "\n",
    "# Plot F1 score\n",
    "plt.plot(results['num_features'], results['f1'], label='F1 Score', marker='.')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Number of Top Features')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Metrics vs. Number of Top Features')\n",
    "plt.legend()\n",
    "\n",
    "# Find the index where each metric is maximized\n",
    "idx_max_accuracy = results['accuracy'].index(max(results['accuracy']))\n",
    "idx_max_precision = results['precision'].index(max(results['precision']))\n",
    "idx_max_recall = results['recall'].index(max(results['recall']))\n",
    "idx_max_f1 = results['f1'].index(max(results['f1']))\n",
    "\n",
    "# Print the number of features and value where each metric is maximized\n",
    "print(f\"Max Accuracy at {results['num_features'][idx_max_accuracy]} features with value {max(results['accuracy'])}\")\n",
    "print(f\"Max Precision at {results['num_features'][idx_max_precision]} features with value {max(results['precision'])}\")\n",
    "print(f\"Max Recall at {results['num_features'][idx_max_recall]} features with value {max(results['recall'])}\")\n",
    "print(f\"Max F1 Score at {results['num_features'][idx_max_f1]} features with value {max(results['f1'])}\")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by importance in descending order\n",
    "sorted_feature_importances = sorted(zip(X.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the names of the top features and their importance scores\n",
    "for feature, importance in sorted_feature_importances:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- ACTION CLASSIFICATION -------------------------------------------------- ###\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_counts = df.groupby('Source Zigbee')['Action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in grouped.index:\n",
    "    if device in devices:\n",
    "        first_value = True\n",
    "        for value in grouped[device]:\n",
    "            if first_value:\n",
    "                print(f\"{device}: {value}\")\n",
    "                first_value = False\n",
    "            else:\n",
    "                print(f\"\\t{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of actions for a certain class and select the rows of the train matrix that correspond to the class\n",
    "\n",
    "# List to keep track of the device class of each train source\n",
    "device_address_list = []\n",
    "class_considered = 'Motion'\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    device_address_list.append(device_type_mapping[train[0]['Source Zigbee']])\n",
    "\n",
    "device_address_series = pd.Series(device_address_list)\n",
    "mask = (device_address_series == class_considered)\n",
    "\n",
    "mask_index = [int(i) for i in mask.index[mask]]\n",
    "\n",
    "action_list = []\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    action_list.append(highest_priority_action)\n",
    "\n",
    "filtered_action_list = [action_list[i] for i in mask.index[mask]]\n",
    "filtered_device_list = [device_address_list[i] for i in mask.index[mask]]\n",
    "\n",
    "matrix_action = matrix_for_binn[mask_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_action_series = pd.Series(filtered_action_list)\n",
    "\n",
    "# Use value_counts() on the Series\n",
    "value_counts_result = filtered_action_series.value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(value_counts_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of actions for a certain class and select the rows of the test matrix that correspond to the class\n",
    "\n",
    "# List to keep track of the device class of each train source\n",
    "test_device_address_list = []\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    test_device_address_list.append(device_type_mapping[train[0]['Source Zigbee']])\n",
    "\n",
    "test_device_address_series = pd.Series(test_device_address_list)\n",
    "mask = (test_device_address_series == class_considered)\n",
    "\n",
    "mask_index = [int(i) for i in mask.index[mask]]\n",
    "\n",
    "test_action_list = []\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    test_action_list.append(highest_priority_action)\n",
    "\n",
    "test_filtered_action_list = [test_action_list[i] for i in mask.index[mask]]\n",
    "test_filtered_device_list = [test_device_address_list[i] for i in mask.index[mask]]\n",
    "\n",
    "test_matrix_action = test_matrix[mask_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_action_series = pd.Series(test_filtered_action_list)\n",
    "\n",
    "# Use value_counts() on the Series\n",
    "value_counts_result = filtered_action_series.value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(value_counts_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and testing on different dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Encode the device address\n",
    "encoder = LabelEncoder()\n",
    "all_action_list = np.concatenate([filtered_action_list, test_filtered_action_list])\n",
    "encoder.fit(all_action_list)\n",
    "encoded_actions = encoder.transform(all_action_list)\n",
    "encoded_Y = encoder.transform(filtered_action_list)\n",
    "encoded_Y_test = encoder.transform(test_filtered_action_list)\n",
    "\n",
    "# Select features to use\n",
    "features_to_use = [0, 1, 2, 3]\n",
    "num_of_features = len(features_to_use)\n",
    "\n",
    "X_train = matrix_action[:, :, features_to_use]\n",
    "y_train = encoded_Y\n",
    "X_test = test_matrix_action[:, :, features_to_use]\n",
    "y_test = encoded_Y_test\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "sampling = False\n",
    "\n",
    "if sampling:\n",
    "    # Convert device_address_list to numpy array for easy indexing\n",
    "    labels = np.array(encoded_Y)\n",
    "\n",
    "    # Identify the unique class labels\n",
    "    unique_classes = np.unique(labels)\n",
    "\n",
    "    # Initialize empty lists to store the balanced dataset\n",
    "    balanced_features = []\n",
    "    balanced_labels = []\n",
    "\n",
    "    # Set the desired number of samples per class\n",
    "    samples_per_class = 200\n",
    "\n",
    "    # Iterate over each class and perform class-wise sampling\n",
    "    for class_label in unique_classes:\n",
    "        # Get indices of instances belonging to the current class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "        # Check if the class has enough samples for sampling\n",
    "        if len(class_indices) >= samples_per_class:\n",
    "            # Sample a fixed number of instances from the current class\n",
    "            sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "            # Append the sampled instances to the balanced dataset\n",
    "            balanced_features.extend(matrix_for_binn[sampled_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[sampled_indices])\n",
    "        else:\n",
    "            # If the class has fewer than 50 samples, include all of them\n",
    "            balanced_features.extend(matrix_for_binn[class_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "    # Convert the balanced dataset to numpy arrays\n",
    "    balanced_features = np.array(balanced_features)\n",
    "    balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "    X_train = balanced_features\n",
    "    y_train = balanced_labels\n",
    "    number_of_epochs = 50\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "#model.add(Bidirectional(LSTM(16)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_actions)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Instantiate ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=number_of_epochs, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "# Load the best model\n",
    "print('Loading best model...')\n",
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "consensus = False\n",
    "occurrence_threshold = 35\n",
    "if consensus:\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "decoded_y_test = encoder.inverse_transform(y_test)\n",
    "decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "classes = sorted(set(test_filtered_action_list))\n",
    "\n",
    "print(classification_report(decoded_y_test, decoded_y_pred_classes, target_names=classes))\n",
    "\n",
    "cm = confusion_matrix(decoded_y_test, decoded_y_pred_classes, normalize='true', labels=classes)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#plt.title('Confusion Matrix first 4 features with sampling')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, f\"{cm[i, j]:.2f}\", ha='center', va='center', color='white' if cm[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"events_bulb.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- TESTING ON DIFFERENT DATASET -------------------------------------------------- ###\n",
    "\n",
    "training_extracted_features = extracted_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X_train = training_extracted_features.drop(columns=['Source Zigbee'])\n",
    "y_train = training_extracted_features['Source Zigbee']\n",
    "\n",
    "X_test = test_extracted_features.drop(columns=['Source Zigbee'])\n",
    "y_test = test_extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# Set the occurrence threshold for label-wise consensus\n",
    "occurrence_threshold = 5\n",
    "occurrences_used = True\n",
    "\n",
    "# Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "for label in np.unique(y_test):\n",
    "    indices = np.where(y_test == label)[0]\n",
    "\n",
    "    # Check if the label has occurred at least occurrence_threshold times\n",
    "    if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "        \n",
    "        # Identify the most common prediction within each occurrence_threshold group\n",
    "        for i in range(0, len(indices), occurrence_threshold):\n",
    "            group_indices = indices[i:i+occurrence_threshold]\n",
    "            most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "            most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[group_indices] = most_frequent_label\n",
    "    else:\n",
    "        most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "        most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "        # Replace all predictions for this label within the group with the most common value\n",
    "        y_pred[indices] = most_frequent_label\n",
    "\n",
    "\n",
    "# Calculate and store metrics for each fold after label-wise consensus\n",
    "accuracy_consensus = accuracy_score(y_test, y_pred)\n",
    "precision_consensus = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall_consensus = recall_score(y_test, y_pred, average='macro')\n",
    "f1_consensus = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('')\n",
    "print(f\"Accuracy: {accuracy_consensus}\")\n",
    "print(f\"Precision: {precision_consensus}\")\n",
    "print(f\"Recall: {recall_consensus}\")\n",
    "print(f\"F1: {f1_consensus}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "unique_mapped_labels = sorted(set(device_name_mapping.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_mapped_labels)\n",
    "disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
